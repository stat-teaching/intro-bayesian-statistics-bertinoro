{
  "hash": "cd8548a0f642f719165a9dff6707df0a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Generalized Linear Models with `brms`\nauthor:\n  - name: Filippo Gambarota\n    orcid: 0000-0002-6666-1747\n    email: filippo.gambarota@unipd.it\n    affiliations: University of Padova\nformat:\n  minimal-revealjs: default\nexecute: \n  echo: false\n  warning: false\nbrand:\n  typography: \n    fonts: \n      - family: Roboto\n        source: google\n        weight: [light, bold]\n        style: [normal, italic]\n    base:\n      family: Roboto\n      weight: light\n    headings: Roboto\nknitr:\n  opts_chunk: \n    fig.align: center\n    dev: svg\n    comment: \"#>\"\nbibliography: \"https://raw.githubusercontent.com/filippogambarota/bib-database/main/references.bib\"\n---\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n# Recap about linear models\n\n## (almost) everything is a linear model\n\nMost of the statistical analysis that you usually perfom, is essentially a linear model.\n\n- The **t-test** is a linear model where a numerical variable `y` is predicted by a factor with two levels `x`\n- The **one-way anova** is a linear model where a numerical variable `y` is predicted by one factor with more than two levels `x`\n- The **correlation** is a linear model where a numerical variable `y` is predicted by another numerical variable `x`\n- The **ancova** is a linear model where a numerical variable `y` is predicted by a numerical variable `x` and a factor with two levels `g`\n- ...\n\n## What is a linear model?\n\nLet's start with a single variable `y`. We assume that the variable comes from a Normal distribution:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## What is a linear model?\n\nWhat we can do with this variable? We can estimate the parameters that define the Normal distribution thus $\\mu$ (the mean) and $\\sigma$ (the standard deviation).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(y)\n#> [1] 100\nsd(y)\n#> [1] 50\n```\n:::\n\n\n## What is a linear model?\n\nUsing a linear model we can just fit a model without predictors, also known as intercept-only model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- glm(y ~ 1, family = gaussian(link = \"identity\"))\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> glm(formula = y ~ 1, family = gaussian(link = \"identity\"))\n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)      100          5      20   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 2500)\n#> \n#>     Null deviance: 247500  on 99  degrees of freedom\n#> Residual deviance: 247500  on 99  degrees of freedom\n#> AIC: 1069.2\n#> \n#> Number of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::\n\n\n## What is a linear model?\n\nI am using `glm` because I want to estimate parameters using Maximul Likelihood, but the results are the same as using `lm`.\n\nBasically we estimated the mean `(Intercept)` and the standard deviation `Dispersion`, just take the square root thus 50.\n\nWhat we are doing is essentially finding the $\\mu$ and $\\sigma$ that maximised the log-likelihood of the model fixing the observed data.\n\n## What is a linear model?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## What is a linear model?\n\nAnd assuming that we know $\\sigma$ (thus fixing it at 50):\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## What is a linear model?\n\nThus, with the estimates of `glm`, we have this model fitted on the data:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Including a predictor\n\nWhen we include a predictor, we are actually try to explain the variability of `y` using a variable `x`. For example, this is an hypothetical relationship:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-9-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\nSeems that there is a positive (linear) relationship between `x` and `y`. We can try to improve the previous model by adding the predictor:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- glm(y ~ x, family = gaussian(link = \"identity\"))\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> glm(formula = y ~ x, family = gaussian(link = \"identity\"))\n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  9.98942    0.09345 106.891  < 2e-16 ***\n#> x            0.41294    0.09829   4.201 5.86e-05 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 0.8668227)\n#> \n#>     Null deviance: 100.249  on 99  degrees of freedom\n#> Residual deviance:  84.949  on 98  degrees of freedom\n#> AIC: 273.48\n#> \n#> Number of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-11-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Assumptions of the linear model\n\nMore practicaly, we are saying that the model allows for varying the mean i.e., each `x` value can be associated with a different $\\mu$ but with a fixed (and estimated) $\\sigma$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-12-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n# Generalized linear models\n\n## Recipe for a GLM\n\n- **Random Component**\n- **Systematic Component**\n- **Link Function**\n\n## Random Component\n\nThe **random component** of a GLM identify the response variable $y$ coming from a certain probability distribution.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-13-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n## Random Component\n\n- In practice, by definition the GLM is a model where the random component is a distribution of the [Exponential Family](https://en.wikipedia.org/wiki/Exponential_family). For example the Gaussian distribution, the Gamma distribution or the Binomial are part of the Exponential Family.\n- These distribution can be described using a **location** parameter (e.g., the mean) and a **scale** parameter (e.g., the variance).\n- The distributions are defined by parameters (e.g., $\\mu$ and $\\sigma$ for the Gaussian or $\\lambda$ for the Poisson). The location (or mean) can be directly one of the parameter or a combination of parameters.\n\n## Random Component, Poisson example\n\nFor example, the [Poisson distribution](https://it.wikipedia.org/wiki/Distribuzione_di_Poisson) is defined as:\n\n$$\nf(k,\\lambda) = Pr(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n$$\n\nWhere $k$ is the number of events and $\\lambda$ (the only parameter) is the *rate*.\n\n## Random Component, Poisson example\n\nThe mean or location of the Poisson is $\\lambda$ and also the scale or variance is $\\lambda$. Compared to the Gaussian, there are no two parameters.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-14-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Random Component\n\nTo sum-up, the random component represents the assumption about the nature of our response variable. **With GLM we want to include predictors to explain *systematic* changes of the mean (but also the scale/variance) of the random component**.\n\nAssuming a Gaussian distribution, we try to explain how the mean of the Gaussian distribution change according to our predictors. For the Poisson, we include predictors on the $\\lambda$ parameters for example.\n\nThe Random Component is called random, beacause it determines how the **error term** $\\epsilon$ of our model is distributed.\n\n## Systematic Component\n\nThe systematic component of a GLM is the combination of predictors (i.e., independent variables) that we want to include in the model.\n\nThe systematic component is also called *linear predictor* $\\eta$ and is usually written in equation terms as:\n$$\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}\n$$\n\nNote that I am omitting the $+ \\epsilon_i$ that you usually find at the end because this is the combination of predictors without errors.\n\n## Systematic Component, an example\n\nAssuming that we have two groups and we want to see if there are differences in a depression score. This is a t-test, or better a linear model, or better a GLM.\n\nIgnoring the random component, we can have a systematic component written in this way:\n\n$$\n\\eta_i = \\beta_0 + \\beta_1{\\mbox{group}_i}\n$$\n\nAssuming that the group is dummy-coded, $\\beta_0$ is the mean of the first group and $\\beta_1$ is the difference between the two groups. In other terms, these are the true or estimated values without the error (i.e., the random component).\n\n## Systematic Component, an example\n\nAnother example, assuming we have the same depression score and we want to predict it with an anxiety score. The blue line is the true/estimated regression line where $\\eta_i$ is the expected value for the observation $x_i$. The red segments are the errors or residuals i.e., the random component.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-15-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Systematic Component\n\nTo sum-up, the systematic component is the combination of predictors that are used to predict the mean of the distribution that is used as random component. The errors part of the model is distributed as the random component.\n\n## Link Function\n\nThe final element is the **link function**. The idea is that we need a way to connect the systematic component $\\eta$ to the random component mean $\\mu$.\n\nThe **link function** $g(\\mu)$ is an **invertible** function that connects the mean $\\mu$ of the random component with the *linear combination* of predictors.\n\nThus $\\eta_i = g(\\mu_i)$ and $\\mu_i = g(\\eta_i)^{-1}$. The systematic component is not affected by $g()$ while the relationship between $\\mu$ and $\\eta$ changes using different link functions.\n\n$$\ng(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}\n$$\n\n$$\n\\mu_i = g(\\eta_i)^{-1} = \\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}\n$$\n\n## Link function\n\nThe simplest **link function** is the **identity link** where $g(\\mu) = \\mu$ and correspond to the standard linear model. In fact, the linear regression is just a GLM with a **Gaussian random component** and the **identity** link function.\n\n\n::: {.cell layout-align=\"center\" tbl-cap='Main distributions and link functions'}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"kvpdbzsuax\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#kvpdbzsuax table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#kvpdbzsuax thead, #kvpdbzsuax tbody, #kvpdbzsuax tfoot, #kvpdbzsuax tr, #kvpdbzsuax td, #kvpdbzsuax th {\n  border-style: none;\n}\n\n#kvpdbzsuax p {\n  margin: 0;\n  padding: 0;\n}\n\n#kvpdbzsuax .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#kvpdbzsuax .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#kvpdbzsuax .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#kvpdbzsuax .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#kvpdbzsuax .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#kvpdbzsuax .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#kvpdbzsuax .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#kvpdbzsuax .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#kvpdbzsuax .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#kvpdbzsuax .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#kvpdbzsuax .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#kvpdbzsuax .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#kvpdbzsuax .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#kvpdbzsuax .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#kvpdbzsuax .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#kvpdbzsuax .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#kvpdbzsuax .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#kvpdbzsuax .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#kvpdbzsuax .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kvpdbzsuax .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#kvpdbzsuax .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#kvpdbzsuax .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#kvpdbzsuax .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kvpdbzsuax .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#kvpdbzsuax .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#kvpdbzsuax .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#kvpdbzsuax .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kvpdbzsuax .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#kvpdbzsuax .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#kvpdbzsuax .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#kvpdbzsuax .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#kvpdbzsuax .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#kvpdbzsuax .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kvpdbzsuax .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#kvpdbzsuax .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kvpdbzsuax .gt_left {\n  text-align: left;\n}\n\n#kvpdbzsuax .gt_center {\n  text-align: center;\n}\n\n#kvpdbzsuax .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#kvpdbzsuax .gt_font_normal {\n  font-weight: normal;\n}\n\n#kvpdbzsuax .gt_font_bold {\n  font-weight: bold;\n}\n\n#kvpdbzsuax .gt_font_italic {\n  font-style: italic;\n}\n\n#kvpdbzsuax .gt_super {\n  font-size: 65%;\n}\n\n#kvpdbzsuax .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#kvpdbzsuax .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#kvpdbzsuax .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#kvpdbzsuax .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#kvpdbzsuax .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#kvpdbzsuax .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#kvpdbzsuax .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#kvpdbzsuax .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#kvpdbzsuax div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" style=\"font-weight: bold;\" scope=\"col\" id=\"Family\">Family</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" style=\"font-weight: bold;\" scope=\"col\" id=\"Link\">Link</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" style=\"font-weight: bold;\" scope=\"col\" id=\"Range\">Range</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"Family\" class=\"gt_row gt_center\"><span data-qmd-base64=\"PGNvZGU+Z2F1c3NpYW48L2NvZGU+\"><span class='gt_from_md'><code>gaussian</code></span></span></td>\n<td headers=\"Link\" class=\"gt_row gt_center\"><span data-qmd-base64=\"aWRlbnRpdHk=\"><span class='gt_from_md'>identity</span></span></td>\n<td headers=\"Range\" class=\"gt_row gt_center\"><span data-qmd-base64=\"JCQoLVxpbmZ0eSwrXGluZnR5KSQk\"><span class='gt_from_md'>$$(-\\infty,+\\infty)$$</span></span></td></tr>\n    <tr><td headers=\"Family\" class=\"gt_row gt_center\"><span data-qmd-base64=\"PGNvZGU+Z2FtbWE8L2NvZGU+\"><span class='gt_from_md'><code>gamma</code></span></span></td>\n<td headers=\"Link\" class=\"gt_row gt_center\"><span data-qmd-base64=\"bG9n\"><span class='gt_from_md'>log</span></span></td>\n<td headers=\"Range\" class=\"gt_row gt_center\"><span data-qmd-base64=\"JCQoMCwrXGluZnR5KSQk\"><span class='gt_from_md'>$$(0,+\\infty)$$</span></span></td></tr>\n    <tr><td headers=\"Family\" class=\"gt_row gt_center\"><span data-qmd-base64=\"PGNvZGU+Ymlub21pYWw8L2NvZGU+\"><span class='gt_from_md'><code>binomial</code></span></span></td>\n<td headers=\"Link\" class=\"gt_row gt_center\"><span data-qmd-base64=\"bG9naXQ=\"><span class='gt_from_md'>logit</span></span></td>\n<td headers=\"Range\" class=\"gt_row gt_center\"><span data-qmd-base64=\"JCRcZnJhY3swLCAxLCAuLi4sIG5fe2l9fXtuX3tpfX0kJA==\"><span class='gt_from_md'>$$\\frac{0, 1, ..., n_{i}}{n_{i}}$$</span></span></td></tr>\n    <tr><td headers=\"Family\" class=\"gt_row gt_center\"><span data-qmd-base64=\"PGNvZGU+Ymlub21pYWw8L2NvZGU+\"><span class='gt_from_md'><code>binomial</code></span></span></td>\n<td headers=\"Link\" class=\"gt_row gt_center\"><span data-qmd-base64=\"cHJvYml0\"><span class='gt_from_md'>probit</span></span></td>\n<td headers=\"Range\" class=\"gt_row gt_center\"><span data-qmd-base64=\"JCRcZnJhY3swLCAxLCAuLi4sIG5fe2l9fXtuX3tpfX0kJA==\"><span class='gt_from_md'>$$\\frac{0, 1, ..., n_{i}}{n_{i}}$$</span></span></td></tr>\n    <tr><td headers=\"Family\" class=\"gt_row gt_center\"><span data-qmd-base64=\"PGNvZGU+cG9pc3NvbjwvY29kZT4=\"><span class='gt_from_md'><code>poisson</code></span></span></td>\n<td headers=\"Link\" class=\"gt_row gt_center\"><span data-qmd-base64=\"bG9n\"><span class='gt_from_md'>log</span></span></td>\n<td headers=\"Range\" class=\"gt_row gt_center\"><span data-qmd-base64=\"JCQwLCAxLCAyLCAuLi4kJA==\"><span class='gt_from_md'>$$0, 1, 2, ...$$</span></span></td></tr>\n  </tbody>\n  \n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n## Gaussian GLM\n\nThus remember that when you do a `lm` or `lmer` you are actually doing a GLM with a Gaussian random component and an identity link function. You are including predictors (systematic component) explaining changes in the mean of the Gaussian distribution.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n::: {.columns}\n::: {.column}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(y ~ x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Coefficients:\n#> (Intercept)            x  \n#>      0.2368       0.3688\n```\n\n\n:::\n:::\n\n\n:::\n::: {.column}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglm(y ~ x, family = gaussian(link = \"identity\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:  glm(formula = y ~ x, family = gaussian(link = \"identity\"))\n#> \n#> Coefficients:\n#> (Intercept)            x  \n#>      0.2368       0.3688  \n#> \n#> Degrees of Freedom: 99 Total (i.e. Null);  98 Residual\n#> Null Deviance:\t    109.6 \n#> Residual Deviance: 96.23 \tAIC: 285.9\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n## Gaussian GLM, a simple simulation\n\nWe can understand the GLM recipe trying to simulate a simple model. Let's simulate a relationship between two numerical variables (like the depression and anxiety example).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nN <- 20\nanxiety <- rnorm(N, 0, 1) # anxiety scores\nb0 <- 0.3 # intercept, depression when anxiety = 0\nb1 <- 0.5 # increase in depression for 1 increase in anxiety\n\n# systematic component\neta <- b0 + b1 * anxiety\n\ndat <- data.frame(anxiety, b0, b1, eta)\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>      anxiety  b0  b1         eta\n#> 1  0.4892000 0.3 0.5  0.54460001\n#> 2 -1.5912630 0.3 0.5 -0.49563148\n#> 3 -0.4770213 0.3 0.5  0.06148934\n#> 4 -1.8916850 0.3 0.5 -0.64584248\n#> 5 -0.4024015 0.3 0.5  0.09879926\n#> 6  0.7335647 0.3 0.5  0.66678234\n```\n\n\n:::\n:::\n\n\n## Gaussian GLM, a simple simulation\n\n`eta` is the linear predictor (without errors):\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-21-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\nThus the expected value of a person with $\\mbox{anxiety} = -1$ is $\\beta_0 + \\beta_1\\times(-1)$ thus -0.2.\n\n## Gaussian GLM, a simple simulation\n\nNow, for a realistic simulation we need some random errors. The random component here is a Gaussian distribution thus each observed (or simulated) value is the systematic component plus the random error $\\mbox{depression}_i = \\eta_i + \\epsilon_i$.\n\nThe errors (or residuals) are assumed to be normally distributed with $\\mu = 0$ and variance $\\sigma^2_{\\epsilon}$ (the residual standard deviation).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsigma <- 1 # residual standard deviation\nerror <- rnorm(N, 0, sigma)\ndepression <- eta + error # b0 + b1 * anxiety + error\n```\n:::\n\n\n## Gaussian GLM, a simple simulation\n\nThis is the simulated dataset. The blue line is the linear predictor and the red segments are the Gaussian residuals.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-23-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Gaussian GLM, a simple simulation\n\nIf we plot the red segments we have roughly a Gaussian distribution. This is the assumption of the GLM with a Gaussian random component.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-24-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## What about the link function?\n\nThe link function for the Gaussian GLM is by default the *identity*. Identity means that $\\eta_i = \\mu_i$, thus there is no transformation. Within each distribution object in R there is the link function and the inverse:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# this is the family (or random component) and the link function. doing a lm() is like glm(family = gaussian(link = \"identity\"))\nfam <- gaussian(link = \"identity\")\nfam$linkfun # link function specifed above\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> function (mu) \n#> mu\n#> <environment: namespace:stats>\n```\n\n\n:::\n\n```{.r .cell-code}\nfam$linkinv # inverse link function\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> function (eta) \n#> eta\n#> <environment: namespace:stats>\n```\n\n\n:::\n:::\n\n\n## What about the link function?\n\nWith the identity, the link function has no effect.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmu <- fam$linkinv(b0 + b1 * anxiety)\nhead(mu)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1]  0.54460001 -0.49563148  0.06148934 -0.64584248  0.09879926  0.66678234\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(fam$linkfun(mu))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1]  0.54460001 -0.49563148  0.06148934 -0.64584248  0.09879926  0.66678234\n```\n\n\n:::\n:::\n\n\nBut with other GLMs, (e.g., logistic regression) the link function is the core element.\n\n## Gaussian GLM, a simple simulation\n\nA more compact (and useful) way to simulate the data is:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndepression <- rnorm(N, mean = fam$linkinv(b0 + b1 * anxiety), sd = sigma)\n```\n:::\n\n\nIn this way is more clear that we are generating data from a normal distribution with fixed $\\sigma^2_{\\epsilon}$ and we are modeling the mean.\n\n# Parameters intepretation\n\n## Parameters intepretation\n\nLet's make a more complex example with a Gaussian GLM with more than one predictor. We have a dataset with 150 observations and some variables.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   depression age group    anxiety\n#> 1 -0.2173456  23    g1  0.0022093\n#> 2  0.3273544  40    g2 -0.1100926\n#> 3  1.0405812  24    g1  0.4261357\n#> 4  2.8139121  33    g2  1.8146185\n#> 5  0.7347837  30    g1 -0.1817395\n#> 6  0.6209260  26    g2 -1.2488724\n```\n\n\n:::\n:::\n\n\nWe want to predict the `depression` with `anxiety`, `group` and `age`.\n\n## Parameters intepretation\n\nLet's fit the model (here using `lm` but is a GLM!):\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = depression ~ anxiety + group + age, data = dat)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -2.0837 -0.6937 -0.1653  0.5869  3.1663 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.229923   0.326055  -0.705  0.48183    \n#> anxiety      0.526979   0.081160   6.493 1.23e-09 ***\n#> groupg2      0.367025   0.165569   2.217  0.02819 *  \n#> age          0.024005   0.009075   2.645  0.00906 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.013 on 146 degrees of freedom\n#> Multiple R-squared:  0.2574,\tAdjusted R-squared:  0.2421 \n#> F-statistic: 16.87 on 3 and 146 DF,  p-value: 1.844e-09\n```\n\n\n:::\n:::\n\n\nHow do you intepret the output? and the model parameters?\n\n# Bayesian Models\n\n## Bayesian vs Frequentists GLM\n\nWhat about the Bayesian version of the previous model? Actually the main difference is that we need to include the priors to obtain posterior distributions about model parameters. The likelihood part is extactly the same as non-bayesian models.\n\n## `brms`\n\nThere are several R packages for estimating Bayesian GLMs. The most complete is called [`brms`](https://paulbuerkner.com/brms/).\n\nThere are also other options such as `rstanarm`. `rstanarm` is faster but less flexible. `brms` include all GLMs (and also other models such as meta-analysis, multivariate, etc.) but is slower and requires more knowledge.\n\nThe syntax is the same as `lm` or `glm` and also `lme4` if you want to include random-effects.\n\n## `brms`\n\nLet's start with a simple model, predicting the `depression` with the group. Thus essentially a t-test:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_group <- brm(depression ~ group, \n                 data = dat, \n                 family = gaussian(link = \"identity\"), \n                 file = here(\"slides/objects/fit_group.rds\"))\n```\n:::\n\n\n## `brms`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(fit_group)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: depression ~ group \n#>    Data: dat (Number of observations: 150) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Regression Coefficients:\n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept     0.56      0.13     0.31     0.82 1.00     3601     2924\n#> groupg2       0.44      0.18     0.08     0.80 1.00     3456     2832\n#> \n#> Further Distributional Parameters:\n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     1.11      0.07     1.00     1.26 1.00     3865     2502\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## `brms` vs `lm`\n\nFirstly, let's compare the two models:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = depression ~ group, data = dat)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -2.7678 -0.7283 -0.1033  0.6176  3.7671 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   0.5795     0.1335   4.341 2.62e-05 ***\n#> groupg2       0.3170     0.1888   1.679   0.0952 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.156 on 148 degrees of freedom\n#> Multiple R-squared:  0.0187,\tAdjusted R-squared:  0.01207 \n#> F-statistic:  2.82 on 1 and 148 DF,  p-value: 0.09521\n```\n\n\n:::\n:::\n\n\n## `brms` results\n\nFirsly we can have a look at the posterior distributions of the parameters:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(fit_group)\n```\n\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-33-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Model checking using simulations\n\nWe can check the model fit using simulations. In Bayesian terms this is called Posterior Predictive Checks. For standard models we use only the likelihood.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-34-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Model checking using simulations\n\nWith the Bayesian models we can just use the `brms::pp_check()` function that compute the posterior predictive checks:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npp_check(fit_group)\n```\n\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-35-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Setting priors\n\nBy default `brms` use some priors. You can see the actual used priors using:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                   prior     class    coef group resp dpar nlpar lb ub\n#>                  (flat)         b                                    \n#>                  (flat)         b groupg2                            \n#>  student_t(3, 0.9, 2.5) Intercept                                    \n#>    student_t(3, 0, 2.5)     sigma                                0   \n#>        source\n#>       default\n#>  (vectorized)\n#>       default\n#>       default\n```\n\n\n:::\n:::\n\n\nYou can also see the priors before fitting the model:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                   prior     class    coef group resp dpar nlpar lb ub\n#>                  (flat)         b                                    \n#>                  (flat)         b groupg2                            \n#>  student_t(3, 0.6, 2.5) Intercept                                    \n#>    student_t(3, 0, 2.5)     sigma                                0   \n#>        source\n#>       default\n#>  (vectorized)\n#>       default\n#>       default\n```\n\n\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n# Centering, re-scaling and contrasts coding\n\n## Centering, re-scaling and contrasts coding\n\nWhen fitting a model is important to transform the predictors according to the hypothesis that we have and the intepretation of parameters.\n\nCentering (for numerical variables) and contrasts coding (for categorical variables) are the two main strategies affecting the intepretation of model parameters.\n\nThe crucial point is that also the prior distribution need to be adapted when using different parametrizations of the same model.\n\n## Rescaling\n\nFor example, let's assume to have the relationship between self-esteem (from 0 to 20) and the graduation mark from 66 to 111 (110 cum laude):\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-39-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Rescaling\n\nLet's fit a simple regression:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: se ~ mark \n#>    Data: dat_mark (Number of observations: 30) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Regression Coefficients:\n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept    -7.96      2.10   -12.20    -3.85 1.00     3159     2822\n#> mark          0.13      0.02     0.09     0.18 1.00     3186     2813\n#> \n#> Further Distributional Parameters:\n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     1.63      0.22     1.27     2.11 1.00     2741     2599\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Rescaling, problems?\n\nWe are using the default priors that are basically non-informative. What about setting appropriate or more informative priors?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                   prior     class coef group resp dpar nlpar lb ub       source\n#>                  (flat)         b                                       default\n#>                  (flat)         b mark                             (vectorized)\n#>  student_t(3, 3.9, 2.5) Intercept                                       default\n#>    student_t(3, 0, 2.5)     sigma                             0         default\n```\n\n\n:::\n:::\n\n\n## Rescaling, problems?\n\nThere are a couple of problems:\n\n- `Intercept` is the expected self-esteem score for people with 0 graduation mark (is that plausible?)\n- `mark` is the expected increase in self-esteem for a unit increase in the graduation mark. (is that intepretable?)\n\nAssuming that we want to put priors, how do you choose the distribution and the parameters?\n\n## Rescaling, problems?\n\nThe first problem is that the `Intercept` is meaningless. Thus we can, for example, mean-center the `mark` variable. The slope is the same, we are only shifting the `x`. The intercept is different.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-42-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Intercept prior\n\nNow the intercept is the expected self esteem value when `mark` is on average. Given that the values ranges from 0 to 20, we could put less probability of extreme values (for average marks, around 88) we could imagine also average value for self-esteem (around 10).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-43-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Intercept prior, centering\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npriors <- c(\n  prior(normal(10, 8), class = \"Intercept\")\n)\n\npriors\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Intercept ~ normal(10, 8)\n```\n\n\n:::\n:::\n\n\n## Slope prior, rescaling\n\nThen for the slope, probably there is too much granularity in the `mark` variable. 1 point increase is very tiny. To improve the model interpretation we can rescale the variable giving more weight to the unit increase.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-45-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Slope prior, rescaling\n\nNow we have a more practical idea of size of the slope. We can use a very vague but not flat prior (the default) considering that 0 means no effect. Remember that now the slope is the increase in self-esteem for incrase of 10 points in the mark.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-46-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Slope prior, rescaling\n\nThe previus prior is very uninformative but is simply excluding impossible values. A slope of 10 means that increasing by 10 points would produce an increase that ranges the entire available scale.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Refitting the model^[For the residual standard deviation, `brms` is already doing a good job with default priors, mainly escluding negative values.]\n\nWe can now refit the model using our priors and rescaling/centering\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: se ~ mark10c \n#>    Data: dat_mark (Number of observations: 30) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Regression Coefficients:\n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept     3.58      0.32     2.95     4.23 1.00     3172     2437\n#> mark10c       0.60      0.21     0.18     0.99 1.00     3775     2708\n#> \n#> Further Distributional Parameters:\n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     1.75      0.24     1.35     2.31 1.00     3378     2792\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Contrasts coding\n\nContrasts coding is a vast and difficult topic. The basic idea is that when you have categorical predictors, with or without interactions, the way you set the contrasts will impact the intepretation of model parameters (and the priors).\n\nBy default in R, categorical variables are coded using the so-called **dummy coding** or **treatment coding**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- factor(rep(c(\"a\", \"b\", \"c\"), each = 5))\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  [1] a a a a a b b b b b c c c c c\n#> Levels: a b c\n```\n\n\n:::\n:::\n\n\n## Contrasts coding\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncontrasts(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   b c\n#> a 0 0\n#> b 1 0\n#> c 0 1\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel.matrix(~x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    (Intercept) xb xc\n#> 1            1  0  0\n#> 2            1  0  0\n#> 3            1  0  0\n#> 4            1  0  0\n#> 5            1  0  0\n#> 6            1  1  0\n#> 7            1  1  0\n#> 8            1  1  0\n#> 9            1  1  0\n#> 10           1  1  0\n#> 11           1  0  1\n#> 12           1  0  1\n#> 13           1  0  1\n#> 14           1  0  1\n#> 15           1  0  1\n#> attr(,\"assign\")\n#> [1] 0 1 1\n#> attr(,\"contrasts\")\n#> attr(,\"contrasts\")$x\n#> [1] \"contr.treatment\"\n```\n\n\n:::\n:::\n\n\n## Contrasts coding\n\nWith a factor with $p$ levels, we need $p - 1$ variables representing contrasts. **dummy-coding** means that there will be a reference level (usually the first level) and $p - 1$ contrasts comparing the other levels with the first level (baseline).\n\nAn example with the `iris` dataset:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlevels(iris$Species)\n#> [1] \"setosa\"     \"versicolor\" \"virginica\"\nfit_dummy <- lm(Sepal.Length ~ Species, data = iris)\nsummary(fit_dummy)\n#> \n#> Call:\n#> lm(formula = Sepal.Length ~ Species, data = iris)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.6880 -0.3285 -0.0060  0.3120  1.3120 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         5.0060     0.0728  68.762  < 2e-16 ***\n#> Speciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\n#> Speciesvirginica    1.5820     0.1030  15.366  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.5148 on 147 degrees of freedom\n#> Multiple R-squared:  0.6187,\tAdjusted R-squared:  0.6135 \n#> F-statistic: 119.3 on 2 and 147 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n## Contrasts coding\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvv <- split(iris$Sepal.Length, iris$Species)\nmean(vv$setosa)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 5.006\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(vv$versicolor) - mean(vv$setosa)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.93\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(vv$virginica) - mean(vv$setosa)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1.582\n```\n\n\n:::\n:::\n\n\n## Contrasts coding\n\nAnother coding scheme could be the so called **Successive Differences Contrast Coding**. The idea is to compare level 2 with level 1, level 3 with level 2 and so on. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\niris$Species_sdif <- iris$Species\ncontrasts(iris$Species_sdif) <- MASS::contr.sdif(3)\ncontrasts(iris$Species_sdif)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                   2-1        3-2\n#> setosa     -0.6666667 -0.3333333\n#> versicolor  0.3333333 -0.3333333\n#> virginica   0.3333333  0.6666667\n```\n\n\n:::\n:::\n\n\n## Contrasts coding\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_sdif <- lm(Sepal.Length ~ Species_sdif, data = iris)\nsummary(fit_sdif)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = Sepal.Length ~ Species_sdif, data = iris)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.6880 -0.3285 -0.0060  0.3120  1.3120 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)      5.84333    0.04203 139.020  < 2e-16 ***\n#> Species_sdif2-1  0.93000    0.10296   9.033 8.77e-16 ***\n#> Species_sdif3-2  0.65200    0.10296   6.333 2.77e-09 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.5148 on 147 degrees of freedom\n#> Multiple R-squared:  0.6187,\tAdjusted R-squared:  0.6135 \n#> F-statistic: 119.3 on 2 and 147 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n## More on contrasts coding\n\nThere are few very useful papers about contrasts coding:\n\n- @Schad2020-ht: comprehensive and (difficult) paper about contrasts coding\n- @Granziol2025-sy: amazing work by our colleagues in Padova\n\n# Hypothesis testing and effect size\n\n## Hypothesis testing\n\nThe easiest way to test an hypothesis similarly to the frequentist framework is by checking if the null value of a certain test is contaned or not in the Credible Interval or the Highest Posterior Density Interval.\n\nIn the frequentist framework, the p value lower than $\\alpha$ corresponds to a confidence interval to $1 - \\alpha$ level that does not contains the null value (e.g., 0).\n\n## `brms::hypothesis()`\n\nThe `brms::hypothesis()` function is a very nice way to test hypotheses into a bayesian framework.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-55-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Bayesian $R^2$ [@Gelman2019-hp]\n\n@Gelman2019-hp explained a generalization of the common $R^2$ to be applied for Bayesian Generalized Linear Models.\n\n$$\n\\text{Bayesian } R^2_s = \n\\frac{\n\\mathrm{Var}_{n=1}^{N}\\left( y_n^{\\text{pred}, s} \\right)\n}{\n\\mathrm{Var}_{n=1}^{N}\\left( y_n^{\\text{pred}, s} \\right) + \\mathrm{Var}_{\\text{res}}^s\n}\n$$\n\nThere are few important points:\n\n- This works for any GLM (unlike the usual $R^2$)\n- Different models on the same dataset cannot be compared <!-- TODO rivedi questo -->\nhttps://avehtari.github.io/bayes_R2/bayes_R2.html\n\n## Extracting posteriors\n\nhttps://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/\nhttps://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/#complete-cheat-sheet\n\n## Bayes Factor\n\nAs an example we can start with the classical coin-flip experiment. We need to guess if a coin is fair or not. Firstly let's formalize our prior beliefs in probabilistic terms:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-56-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Bayes Factor\n\nNow we collect data and we observe $x = 40$ tails out of $k = 50$ trials thus $\\hat{\\pi} = 0.8$ and compute the *likelihood*:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-57-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Bayes Factor\n\nFinally we combine, using the Bayes rule, **prior** and **likelihood** to obtain the **posterior** distribution:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-58-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Bayes Factor\n\nThe **Bayes Factor** (BF) --- also called the **likelihood ratio**, for obvious reasons --- is a measure of the relative support that the evidence provides for two competing hypotheses, $H_0$ and $H_1$ (~ $\\pi$ in our previous example).  It plays a key role in the following *odds form* of Bayes's theorem.\n\n$$\n\\frac{p(H_0|D)}{p(H_1|D)} = \\frac{p(D|H_0)}{p(D|H_1)} \\times \\frac{p(H_0)}{p(H_1)}\n$$\n\nThe ratio of the priors $\\frac{p(H_0)}{p(H_1)}$ is called the **prior odds** of the hypotheses; and, the ratio of the poosteriors $\\frac{p(H_0| D)}{p(H_1 | D)}$ is called the **posterior odds** of the hypotheses. Thus, the above (odds form) of Bayes's Theorem can be paraphrased as follows\n\n$$\n\\text{posterior odds} = \\text{Bayes Factor} \\times \\text{prior odds}\n$$\n\n## Calculating the Bayes Factor using the SDR\n\nCalculating the BF can be challenging in some situations. The Savage-Dickey density ratio (SDR) is a convenient shortcut to calculate the Bayes Factor [@Wagenmakers2010-fj]. The idea is that the ratio of the prior and posterior density distribution for hypothesis $H_1$ is an estimate of the Bayes factor calculated in the standard way.\n    \n$$\nBF_{01} = \\frac{p(D|H_0)}{p(D|H_1)} \\approx \\frac{p(\\pi = x|D, H_1)}{p(\\pi = x | H_1)}\n$$\n\nWhere $\\pi$ is the parameter of interest and $x$ is the null value under $H_0$ (e.g., 0). and $D$ are the data. \n\n## Calculating the Bayes Factor using the SDR\n\nFollowing the previous example $H_0: \\pi = 0.5$. Under $H_1$ we use a vague prior by setting $\\pi \\sim Beta(1, 1)$.\n\nSay we flipped the coin 20 times and we found that $\\hat \\pi = 0.75$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-59-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Calculating the Bayes Factor using the SDR\n\nThe ratio between the two black dots is the Bayes Factor.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-60-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Bayes Factor in `brms`\n\nhttps://vuorre.com/posts/2017-03-21-bayes-factors-with-brms/\nhttps://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html#:~:text=For%20the%20computation%20of%20Bayes,flat%20priors%20the%20null%20is\n\n## Be careful with the Bayes Factor\n\nThe Bayes Factor computed with `hypothesis()` or in general the SDR method is highly sensitive to the priors. See also the `bayestestR` [documentation](https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html#setting-the-correct-prior). In general:\n\n- The Bayes Factor requires informative or at least non-flat priors. Remember that in `brms` the default prior is flat\n- As the prior scale (e.g., standard deviation) increase, the Bayes Factor tends to suggest evidence for the null hypothesis, even when the null hypothesis is false\n\nThis is called [Lindley's paradox](https://en.wikipedia.org/wiki/Lindley%27s_paradox). [see @Wagenmakers2023-ll].\n\n## Lindley's paradox, a simulation\n\nWe can just simulate a t-test (or equivalently a parameter of a model) and run the Bayesian model with different prior distribution scale.\n\nFor a t-test we are focused on the prior for the (standardized) difference between the means. We can set a prior centered on 0 with different scale.\n\nHere I'm using the `BayesFactor` package just because is faster for simple model if we are interested in the Bayes Factor pointnull (the same as `brms::hypothesis(x, \"b = 0\")`).\n\n## Lindley's paradox, a simulation\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-61-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Prior Sensitivity Check\n\nGiven the sensitivity of Bayesian models to the prior, especially when informative is always a good idea to check the actual impact of priors. Not only for Bayes Factors but also for the posterior distributions.\n\n- Run your model with the prior that are more plausible for you\n- Run the same model with priors that are more and less informative\n- Check the impact of the prior on your posterior distributions and conclusions\n\n## Prior Predictive Check\n\nThe prior predictive check is an important process when setting the priors in a regression model. The basic idea is that the posterior distribution is the combination between priors and likelihood (i.e., the data). If we remove likelihood from the equation, we obtain a \"posterior\" only using information in the prior.\n\nThe advantage is that we can simulate data to check if our priors are reasonable according to our model, phenomenon, etc.\n\nIn `brms` there is an argument called `sample_prior` that can be set to `\"only\"` to fit a model ignoring the data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbrm(\n  y ~ x,\n  data = dat, # not used\n  sample_prior = \"only\"\n)\n```\n:::\n\n\n## Prior predictive check\n\nAssuming to have a simple model with a continous and categorical predictor:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            y          x g\n#> 1  0.7812706  0.6207567 0\n#> 2 -1.1101886  0.0356414 0\n#> 3  0.3243795  0.7731545 0\n#> 4  1.8243161  1.2724891 0\n#> 5  0.6446891  0.3709754 0\n#> 6  1.0857640 -0.1628543 0\n```\n\n\n:::\n:::\n\n\n## Prior predictive check\n\nWe know that `x` and `y` are standandardized thus we can think about the effect `g` in terms of Cohen's $d$ and the effect of `x` in terms of units of standard deviations. Let's set some priors for $\\beta_1$ and $\\beta_2$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_prior(y ~ x + g, data = dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                 prior     class coef group resp dpar nlpar lb ub       source\n#>                (flat)         b                                       default\n#>                (flat)         b    g                             (vectorized)\n#>                (flat)         b    x                             (vectorized)\n#>  student_t(3, 0, 2.5) Intercept                                       default\n#>  student_t(3, 0, 2.5)     sigma                             0         default\n```\n\n\n:::\n:::\n\n\n## Prior predictive check\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npriors <- c(\n  prior(normal(0, 1), class = \"b\", coef = \"g\"),\n  prior(normal(0, 2), class = \"b\", coef = \"x\")\n)\n\nfit_prior <- brm(y ~ x + g, data = dat, sample_prior = \"only\", prior = priors, file = here(\"slides\", \"objects\", \"fit_prior.rds\"))\nsummary(fit_prior)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: y ~ x + g \n#>    Data: dat (Number of observations: 50) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Regression Coefficients:\n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept     0.30      5.00    -8.67     8.82 1.00     2585     1590\n#> x             0.03      2.04    -4.00     3.96 1.00     2844     2449\n#> g            -0.02      1.00    -2.05     1.91 1.00     3089     2397\n#> \n#> Further Distributional Parameters:\n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     2.79      3.83     0.08    10.10 1.00     2647     1440\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Prior predictive check\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-66-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Extracting Posterior Distributions\n\nWhen running a model with `lm` and `glm`, all the required information is included into the summary of the model. For each parameter we have the point estimate, standard error, confidence interval, etc.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_lm_example <- lm(Sepal.Length ~ Petal.Width + Species, data = iris)\nsummary(fit_lm_example)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = Sepal.Length ~ Petal.Width + Species, data = iris)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.3891 -0.3043 -0.0472  0.2528  1.3358 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)        4.78044    0.08308  57.543  < 2e-16 ***\n#> Petal.Width        0.91690    0.19386   4.730 5.25e-06 ***\n#> Speciesversicolor -0.06025    0.23041  -0.262    0.794    \n#> Speciesvirginica  -0.05009    0.35823  -0.140    0.889    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.481 on 146 degrees of freedom\n#> Multiple R-squared:  0.6694,\tAdjusted R-squared:  0.6626 \n#> F-statistic: 98.53 on 3 and 146 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n## Extracting Posterior Distributions\n\nWith Bayesian models we have posterior distributions that can be summarised in different ways. There are several methods and packages to work with posteriors:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Extracting Posterior Distributions\n\nWith the `as_draws_df` we can extract all the samples from the posterior distribution:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A draws_df: 6 iterations, 1 chains, and 6 variables\n#>   b_Intercept b_mark sigma Intercept lprior lp__\n#> 1        -9.6   0.15   1.5       4.3   -3.4  -59\n#> 2       -10.2   0.16   1.4       4.1   -3.3  -59\n#> 3        -9.0   0.15   1.5       4.6   -3.4  -62\n#> 4        -9.0   0.15   1.4       4.3   -3.4  -60\n#> 5        -8.7   0.14   1.5       3.8   -3.4  -59\n#> 6        -8.0   0.13   1.4       3.7   -3.3  -60\n#> # ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n```\n\n\n:::\n:::\n\n\n## Extracting Posterior Distributions\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas_draws_df(fit_mark) |> \n  select(starts_with(\"b\"), sigma) |> \n  posterior::summarise_draws()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 3  10\n#>   variable     mean median     sd    mad       q5    q95  rhat ess_bulk ess_tail\n#>   <chr>       <dbl>  <dbl>  <dbl>  <dbl>    <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n#> 1 b_Interce -7.96  -7.98  2.10   2.05   -11.5    -4.49   1.00    3049.    2798.\n#> 2 b_mark      0.133  0.133 0.0230 0.0227   0.0954  0.172  1.00    3064.    2776.\n#> 3 sigma       1.63   1.60  0.217  0.211    1.32    2.03   1.00    2706.    2588.\n```\n\n\n:::\n:::\n\n\n## HPDI with `bayestestR`\n\nThe `bayestestR` contains a lot of functions to do inference and post-processing on `brms` (and also other) models:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbayestestR::hdi(fit_mark)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Highest Density Interval\n#> \n#> Parameter   |         95% HDI\n#> -----------------------------\n#> (Intercept) | [-12.24, -3.92]\n#> mark        | [  0.09,  0.18]\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(bayestestR::hdi(fit_mark))\n```\n\n::: {.cell-output-display}\n![](bayesian-glm_files/figure-revealjs/unnamed-chunk-71-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## The `broom` package\n\nThe `broom.mixed` package automatically provide the `summary()` into a data.frame format:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(broom.mixed)\nbroom.mixed::tidy(fit_mark)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 3  8\n#>   effect   component group    term         estimate std.error conf.low conf.high\n#>   <chr>    <chr>     <chr>    <chr>           <dbl>     <dbl>    <dbl>     <dbl>\n#> 1 fixed    cond      <NA>     (Intercept)    -7.96     2.10   -12.2       -3.85 \n#> 2 fixed    cond      <NA>     mark            0.133    0.0230   0.0882     0.179\n#> 3 ran_pars cond      Residual sd__Observa    1.63     0.217    1.27       2.11\n```\n\n\n:::\n:::\n\n\n## More on `bayestestR`\n\nThis is a really amazing package, have a look at the `Features` section because contains functions and explanations about different ways of summarizing the posterior distribution.\n\n## Bayesian Workflow\n\n- Nice paper about the Bayesian Workflow: [https://sites.stat.columbia.edu/gelman/research/unpublished/Bayesian_Workflow_article.pdf](https://sites.stat.columbia.edu/gelman/research/unpublished/Bayesian_Workflow_article.pdf)\n\n## References {.refs}",
    "supporting": [
      "bayesian-glm_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}