\documentclass[compress,serif]{beamer}  % t per mettere al top delle slide
%\documentclass[compress,serif,handout]{beamer} 
%%% Dichiarazione dei pacchetti standard.
\usepackage[italian]{babel}
\usepackage{tikz}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{bm}
\usepackage{amsmath,amssymb}

%%% Personalizzazione del layout---articolata su cinque livelli.
\definecolor{cambridgedarkblue}{RGB}{0,62,114}
\definecolor{cambridgedarkorange}{RGB}{200,78,0}
\definecolor{cambridgedarkblue1}{RGB}{0,55,102}
\definecolor{cambridgedarkblue2}{RGB}{0,49,91}
%\def\hilite<#1>{%
 %   \temporal<#1>{\color{gray}}{\color{black}}%
  %  {\color{black}}}

%%% Personalizzazione del layout---articolata su cinque livelli.
\usetheme{Frankfurt}        % layout complessivo.
\usefonttheme{structurebold}
%%%% page number
\setbeamertemplate{footline}[frame number]
\setbeamerfont{footline}{size=\scriptsize, series=\bfseries}
\setbeamercolor{footline}{fg = black, bg = black}
\setbeamercolor{page number in head/foot}{fg = gray}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usecolortheme[named=cambridgedarkblue]{structure}
\setbeamercolor{palette primary}{bg=cambridgedarkblue,fg=white}
\setbeamercolor{palette secondary}{bg=cambridgedarkblue,fg=white}
\setbeamercolor{palette tertiary}{bg=white,fg=cambridgedarkblue}
\setbeamercolor{palette quaternary}{fg=white,bg=cambridgedarkblue}
\setbeamercolor{uppercolor}{fg=orange!80!white, bg=cambridgedarkblue2}
\setbeamercolor{lowercolor}{fg=white, bg=cambridgedarkblue1}
\setbeamercolor{zen}{fg=orange!40!blue,bg=white!20!yellow}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
<<include=FALSE>>=
library(knitr)
opts_chunk$set(fig.width=4, fig.height=3,echo=FALSE,dev="tikz",fig.align='center',results="hide",comment=NA,prompt=TRUE,cache=TRUE,external = FALSE)
opts_knit$set(out.format = "latex") #,width=30)
thm = knit_theme$get("autumn")  # approved by lptrainer
knit_theme$set(thm)

@
% \setkeys{Gin}{width=\textwidth}
% \renewenvironment{knitrout}{\setlength{\topsep}{1mm}}{} % elimina lo spazio prima del chunk
<<echo=FALSE,message=FALSE,warning=FALSE,cache=FALSE>>=
rm(list=ls())
options(width=45)
main <- "~/didattica/Bertinoro/"
if (grepl("kolmogorov",Sys.info()["user"])) main <- gsub("~/","~/MEGAsync/",main)
if (grepl("cox",Sys.info()["user"])) main <- gsub("~/","~/MEGA/",main)
datadir <- paste(main,"data/",sep="")
Rdir <- paste(main,"Rcodes/",sep="")
Sdir <- paste(main,"Scodes/",sep="")
esegui <- FALSE
# KUtils::pulizia(paste(main,"knitr/",sep=""), c(".Rnw",".bib"))
# KUtils::printslide("Bayes0.Rnw",paste(main,"knitr/",sep=""),pulisci=FALSE)

@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Titolo e autore.
\title{\textbf{2. A Brief IntRoduction to
Probability and Probability Distributions}}
\author{\textbf{Massimiliano Pastore (\& Gianmarco Altoè)}\\
   University of Padova}
\date{}

\begin{document}

\begin{frame}[plain]
  \titlepage
\end{frame}
          

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Contents}
  \tableofcontents
\end{frame}


%% Aggiungere definizioni di probabilità

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\subsection{}
\begin{frame}{\textbf{Introduction}}

\begin{itemize}
 \item In very general terms, probability could be viewed as a mesaure of uncertainty related to the occurrence of an event 
 \item The theory of probability is the basis for inferential statistics (both frequentist and bayesian inferential statistics)
 \item So before moving to bayesian inference, let's recall the basic properties of probability
\end{itemize}

\footnotesize \vspace{.5cm}
Note. For a more in depth introduction to the theory of probability see: Etz \& Vandekerckhove (2017). Introduction to Bayesian inference for psychology. \emph{Psychonomic Bulletin \& Review}, 1-30
\url{https://link.springer.com/article/10.3758/s13423-017-1262-3}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Basics: Sample Space, Outcomes and Events}}

\begin{itemize}
 \item Given a random phenomenon (e.g., a roll of a die), the set of all possible \emph{outcomes} (results) is called \textbf{sample space} ($S$)
 \item The elements of $S$ must be \emph{mutually exclusive} and \emph{collectively exhaustive}
 \item An \emph{event} ($E$) is a subset of $S$
 
 \item Example: \\
 \emph{Random phenomenon}: A roll of a six-sided dice \\
 \emph{Sample space}: $S = \{1, 2, 3, 4, 5, 6\}$ \\
\emph{Event}: “An even number will come up”, $E=\{2,4,6\}$
 
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Discrete and continuous sample spaces}}

\begin{itemize}
 \item A sample space is called \textbf{discrete} if its elements are countable: \\ \vspace{.3cm}
  EXAMPLE \\
  \emph{Phenomenon}: Number of errors in a task \\
  \emph{Sample space}: $S = \{0, 1, 2, 3, \ldots \}$
 
 \vspace{.3cm}
 \item A sample space is called \textbf{continuous} if its elements are not countable, but rather they represent a continuum of values \\ \vspace{.3cm}
EXAMPLE \\
\emph{Phenomenon}: Proportion of fixation time to a stimulus ($\theta$) \\
 \emph{Sample space}: $S = \{0 \leq \theta \leq 1\}$
 
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\section{Probability definitions}
\begin{frame}{\textbf{Probability definitions}}

\begin{itemize}
 \item The first formal approaches to probability theory were made in the second half of the 17th century (Pascal, Fermat, Bernoulli). 
 \item The first areas of application for probability theory were in the fields of games of chance and insurance problems.
 \item One first attempt at rigorous formalization was made by Laplace (1812); other important developments were made by De Moivre (1667--1754), Gauss (1777--1855) and Poisson (1781--1840).
 \item From the second half of the 19th century until the 1920s, important contributions were made by Chebyshev (1821--1894), Markov (1856--1922), and Lyapunov (1857--1918).
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Probability definitions}}

\begin{enumerate}
 \item Classic (Laplace)
 \item Frequentist (Von Mises, Reichebach, Castelnuovo)
 \item Subjective (De Finetti, Ramsey, Savage)
 \item Axiomatic (Kolmogorov)
\end{enumerate}

\vspace{.3cm}
\begin{columns}
\begin{column}{.1\textwidth}
\end{column}
\begin{column}{.3\textwidth}
  \scalebox{.23}{\includegraphics{img/vonMises.jpeg}}
\end{column}
\begin{column}{.3\textwidth}

  \scalebox{.64}{\includegraphics{img/Bruno_de_Finetti.jpeg}}
\end{column}
\begin{column}{.3\textwidth}
  \scalebox{.28}{\includegraphics{img/Kolmogorov.jpg}}
\end{column}
\end{columns}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Classical definition}}

\small
The probability of an event $E$ is the ratio of the number of cases favorable to its occurrence ($s$) to the total number of possible cases ($n = |S|$), assuming that all outcomes are equally probable.

\vspace{.3cm}
Formally: $$p(E) = \frac{s}{|S|}$$

where $|S|$ is the cardinality of the sample space $S$. 

\vspace{.3cm}
\textbf{Limitations}: 

\begin{itemize}
 \item It is not always possible to determine $|S|$, the total number of possible outcomes in the sample space, especially for infinite or very large sample spaces.
 \item The assumption that all possible outcomes are equally probable  does not hold in many real-world scenarios.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Frequentist definition}}

\small
The probability of an event $E$ is defined as the value to which the relative frequency of occurrence of the event tends to converge as the number of trials approaches infinity

\vspace{.3cm}
Formally: $$p(E) = \lim_{n \to \infty} \frac{f(E)}{n} $$

where $f(E)$ is the number of times event $E$ occurs in $n$ trials.

\vspace{.3cm}
\textbf{Limitations}: 

\begin{itemize}
 \item It is not possible to determine the exact probability, only an estimate based on observed frequencies.
 \item The reliability of the estimate depends on tests being performed under consistent conditions; if conditions vary, the estimate may not be reliable.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Subjective definition}}

The subjective definition of probability was developed to address the limitations of classical and frequentist approaches. According to this definition:

\vspace{.3cm}
The probability of an event is the price that an individual considers fair to pay for a contract that pays:

\begin{itemize}
  \item  1 unit of currency if the event occurs
  \item  0 if the event does not occur
\end{itemize}

\vspace{.3cm}
This definition reflects the degree of belief an individual has in the occurrence of the event, based on their personal knowledge and judgment.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Assiomatic definition}}

\small  
The axiomatic definition of probability is not operational in nature. Unlike classical or frequentist definitions, it does not provide specific guidance on how to calculate probabilities. Instead, it establishes a formal mathematical framework that can accommodate both objectivist and subjectivist interpretations of probability.

\vspace{.3cm}
The term 'axiomatic' refers to the process of axiomatization used in its development. This process involves:

\begin{itemize}
  \item  Identifying primitive (undefined) concepts in probability theory.
  \item  Establishing a set of axioms or postulates based on these concepts.
    Deriving theorems and properties of probability from these axioms.
\end{itemize}

\vspace{.3cm}
This approach provides a rigorous mathematical foundation for probability theory, allowing for consistent application across various fields and interpretations.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Summarizing}}

\begin{itemize}
 \item \textbf{Probability theory} serves as a general system for representing plausibility. It applies both to enumerable events in the world (by counting the ways in which they can occur) and to theoretical constructs defined by \textbf{parameters} (McElreath, 2016).
\end{itemize}
\begin{beamercolorbox}[rounded=true, shadow=true]{formule}
\begin{itemize} \vspace{-.3cm}
 \item \textbf{Probability is a measure of the degree of uncertainty about the occurrence of an event.}
\end{itemize}
\end{beamercolorbox}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{The probability function}}

The probability function, $Pr()$, is a mathematichal function that assigns numbers to each outcome of a sample space. \\
The numbers, called probabilities, need to satisfy three properties (Kolmogorov, 1956):
 \begin{enumerate}
   \item A probability value must be non-negative (i.e., zero or positive)
   \item The sum of the probabilities across all outcomes in the entire sample space must be 1
   \item For any two mutually exclusive events, the probability that one or the other occurs is the sum of their individual probabilities
  \end{enumerate}
  
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probability distributions}
\begin{frame}[plain]

\begin{beamerboxesrounded}[upper=title, lower=structure, shadow=true]
    {\center{\Large{\textbf{Probability distributions}}}}
\end{beamerboxesrounded}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{frame}{\textbf{Probability distribution}}

A probability distribution is a list of all outcomes of the sample space and their corresponding probabilities \\ \vspace{.3cm}

\textbf{Example} \begin{itemize}
 \item \textbf{Phenomenon}: $X$ = a single toss of a fair coin; where $X$ is a \emph{random variable} that can take the following values: $x = 0$ (Head) e $x = 1$ (Tail)
 \item \textbf{Sample space}: $S = \{0, 1\}$
 \item \textbf{Probability distribution}: $Pr(X = x) = \theta^x (1- \theta)^{1-x}$ \\
 where $\theta$ (i.e., the parameter that defines the probability distribution) is equal to .5
 \item \textbf{Properties of the probability distribution}: \\
$Pr(X = 0) = .5$; $Pr(X = 1) = .5 \geq 0$
$Pr(X = 0) + Pr(X = 1) = 1$
\end{itemize}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Discrete and continuous probability distributions}}

\begin{itemize}
 \item A probability distribution associated with a \emph{discrete sample space} is called \textbf{discrete probability distribution} or \textbf{mass probability function}
 \item A probability distribution associated with a \emph{continuous sample space} is called \textbf{continuous probability distribution} or \textbf{density probability function}
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Properties of discrete probability distributions}}

Let $Pr(X = x)$ be a discrete probability distribution with sample space $S = \{X\}$, then:

\begin{eqnarray}
 0 \leq Pr(X = x) \leq 1 \makebox[.5cm][c]{} \forall x
\end{eqnarray}

\begin{eqnarray}
\sum_X Pr(X = x) = 1 
\end{eqnarray}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Some discrete probability distributions}}


<<>>=
dd <- data.frame(Name = c("Bernoulli","Binomial","Poisson"), S = c("$x = 0,1$","$x = 0,1,\\ldots,n$","$x = 0,1,2,\\ldots$"), Param. = c("$\\theta$","$\\theta$","$\\lambda$"),Mass=c("$\\theta^x(1-\\theta)^{1-x}$","$\\left( \\begin{array}{c} n \\\\ x \\end{array} \\right) \\theta^x(1-\\theta)^{n-x}$","$e^{-\\lambda} \\frac{\\lambda^n}{n!}$"), Mean = c("$\\theta$","$n\\theta$","$\\lambda$"), Variance = c("$\\theta(1-\\theta)$","$n\\theta(1-\\theta)$","$\\lambda$"))
@

<<results='asis'>>=
library(xtable)
print(xtable(dd,align = "lllcccc"), include.rownames=FALSE, sanitize.text.function = function(x){x},size="scriptsize")
@


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Properties of continuous probability distributions}}

Let $Pr(X = x)$ be a continuous probability distribution with sample space $S = \{X\}$, then:

\begin{eqnarray}
  p(X = x) \geq 0 \makebox[.5cm][c]{} \forall x
\end{eqnarray}

\begin{eqnarray}
\int_X p(X = x)dx = 1 
\end{eqnarray}


\footnotesize \vspace{.3cm} \pause
Note. Probability densities can be greater than 1, whereas probability masses cannot be greater than 1. In any case, the overall area under the probability function (i.e. the overall probability associated with the sample space) is always 1. \\
$\ldots$ try 
<<echo=TRUE, eval=FALSE>>=
curve( dnorm(x,mean=0,sd=.1), from=-.5, to=.5 )
@


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Some continuous probability distributions}}


<<>>=
cc <- data.frame(Name = c("Uniform","Normal","Beta"), 
                 S = c("$a \\leq x \\leq b$","$-\\infty \\leq x \\leq +\\infty$","$0 \\leq x \\leq 1$"), Param. = c("$a,b$","$\\mu,\\sigma$","$a,b$"),Density=c("$\\frac{1}{b-a}$","$\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2}}$","$\\frac{x^{(a-1)}(1-x)^{(b-1)}}{B(a,b)}$"), Mean = c("$\\frac{a+b}{2}$","$\\mu$","$\\frac{a}{a+b}$"), Variance = c("$\\frac{(b-a)^2}{12}$","$\\sigma^2$","$\\frac{ab}{(a+b)^2 (a+b+1)}$"))
@

<<results='asis'>>=
print(xtable(cc,align = "lllcccc"), include.rownames=FALSE, sanitize.text.function = function(x){x},size="scriptsize")
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Probability distributions and R}}

\begin{itemize}
 \item R makes it easy to work with probability distributions
 \item There are four functions available for probability
distributions: \begin{itemize}
      \item \texttt{d*}: probability mass for discrete distribution and probability density for continuous distributions
      \item \texttt{p*}: the cumulative probability for a given quantile
      \item \texttt{q*}: the quantile for a given cumulative probability
      \item \texttt{r*}: draw a random sample for a given distribution
    \end{itemize}
\end{itemize}

\vspace{.3cm}
Note. \texttt{*} distribution names in R (e.g., Normal: \texttt{norm})
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Some probability distributions in R}}

<<>>=
dr <- data.frame(
  Distribution = c("Binomial","Poisson","Uniform","Normal","Student's $t$","Beta"),
  Type = c(rep("discrete",2),rep("continuous",4)),
  Name = c("\\texttt{@binom}","\\texttt{@pois}","\\texttt{@unif}","\\texttt{@norm}","\\texttt{@t}","\\texttt{@beta}")
)
@

<<results='asis'>>=
print(xtable(dr),sanitize.text.function=function(x){x}, include.rownames=FALSE)
@

\vspace{.3cm}
Note:
\texttt{@} = prefix: \texttt{d} or \texttt{p} or \texttt{q} or \texttt{r} \\
For example \texttt{rnorm()} draw random samples from a Normal distribution
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{$\ldots$ from theory to practice}}

Let's practice with some probability distributions:

\begin{itemize}
 \item Poisson
 \item Normal
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{frame}{\textbf{Poisson distribution}}

Suppose that the number of errors committed in a cognitive
task has a Poisson distribution with parameter $\lambda = 3$:


$$Pr(X = x) \sim P(x; \lambda), \makebox[.5cm][c]{} x = 0, 1, 2, 3, 4, 5, \ldots$$


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{\textbf{Graphical representation}}

<<echo=TRUE,size="small",eval=FALSE>>=
x <- 0:14
plot( x, dpois( x, lambda = 3), type = "h", ylab = "Pr(x)" )
points( x, dpois( x, lambda = 3 ), pch = 19 )
@
\pause \vspace{-1cm}
<<>>=
x <- 0:14
plot( x, dpois( x, lambda = 3), type = "h", ylab = "$Pr(x)$" , xlab="$x$")
points( x, dpois( x, lambda = 3 ), pch = 19 )
@
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{\textbf{Exercises}}

\scriptsize
\begin{itemize} \pause
  \item What is the probability that a subject makes 4 errors? \pause
<<echo=TRUE,results='markup',size="scriptsize">>=
dpois( 4, 3 )
@
  \pause 
  \item What is the probability that a subject makes 3 or 4 errors? \pause
<<echo=TRUE,results='markup',size="scriptsize">>=
dpois( 3, 3 ) + dpois( 4, 3 )
@
  \pause
  \item What is the probability that a subject makes less than 2 errors? \pause
<<echo=TRUE,results='markup',size="scriptsize">>=
ppois( 1, 3 ) 
@
  \pause
  \item What is the probability that a subject makes more than 2 errors? \pause
<<echo=TRUE,results='markup',size="scriptsize">>=
1 - ppois( 2, 3 ) 
@
  \pause
  \item What is the probability that a subject makes at least 1 error? \pause
<<echo=TRUE,results='markup',size="scriptsize">>=
1 - ppois( 0, 3 ) 
@
  
\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{\textbf{Drawing random samples}}

A random samples of 10 performance \pause
<<echo=TRUE,results='markup'>>=
rpois( 10, 3 )
@
\pause

A random samples of 1000000 performance \pause
<<echo=TRUE,results='markup'>>=
large_sample <- rpois( 1e6, 3 )
@
\pause

Sample mean and sample variance \pause

<<echo=TRUE,results='markup'>>=
mean( large_sample )
var( large_sample )
@

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{frame}{\textbf{Normal distribution}}

Suppose that the time (in seconds) spent on by a guinea pig
to complete a maze is normally distributed with mean
$\mu = 180$ e standard deviation $\sigma = 40$:


$$p(X = x) \sim \mathcal{N}(x; 180, 40)$$

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{\textbf{Graphical representation}}

<<echo=TRUE,fig.keep='none',size="small">>=
curve( dnorm( x, 180, 40 ), from = 180-40*4, to = 180+40*4,
       ylab = "p(x)" )
@
\pause \vspace{-1cm}
<<>>=
curve( dnorm( x, 180, 40 ), from = 180-40*4, to = 180+40*4, ylab = "$p(x)$", xlab="$x$" )
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Exercises}}

\begin{enumerate} 
 \item What is the probability that a randomly selected guinea pig spends less than 200 seconds to complete the maze?
 \item What is the probability that a randomly selected guinea pig spends between 140 and 220 seconds to complete the maze?
 
 \item Graphically represent 3 normal distributions with a fixed mean of 180 and a standard deviation of 20, 40, 80 respectively. \\
\small
Hint: To add different graphs on the same plot use the option \texttt{add = TRUE} (see, \texttt{?curve})
\end{enumerate}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bivariate probability distributions}
\begin{frame}[plain]

\begin{beamerboxesrounded}[upper=title, lower=structure, shadow=true]
    {\center{\Large{\textbf{Bivariate probability distributions}}}}
\end{beamerboxesrounded}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Bivariate probability distributions}}

\begin{itemize}
 \item There are many situations in which we are interested in the conjunction of two (or more) outcomes regarding two(or more) random variables 
 \item For example:  
 \begin{itemize}
  \item What is the probability of meeting a person with both red hair and green eyes?
 \item What is the probability that “Linda is a bank teller and is active in the feminist movement?” (Kahneman e Tversky, 1974)  
\end{itemize}
 \item \textbf{Bivariate} (or \textbf{multivariate}) probability distributions are used to answer these kinds of questions
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{frame}{\textbf{Tossing a fair coin}}

As an example for developing our new ideas, imagine tossing a fair coin three times in a row: 

<<>>=
library(gtools)
P <- permutations( 2, 3, c("H","T"), repeats.allowed = TRUE )
dcoin <- data.frame(Sample_space = apply( P, 1, function(x){paste(x,collapse="")}), Probability=rep("1/8",8))
@

<<results='asis'>>=
print(xtable(dcoin,align="ccc"),include.rownames = FALSE)
@
\footnotesize
\centerline{Note. T = Tail, H = Head}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Let's now consider two random variables}}

\begin{itemize}
 \item  $X = $ number of heads, where: \\ $S_X = \{0,1,2,3\}$
 \vspace{.3cm} \item $Y = $ Number of switches between heads and tails,
where: \\ $S_X = \{0,1,2\}$
\end{itemize}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Table of the bivariate probability distribution}}

The bivariate (or conjoint) probability distribution of $X$ and $Y$: $$Pr(X = x, Y = y) = Pr(x, y)$$

<<>>=
dXYvuoto <- dXY <- data.frame( Y = 0:2, X0 = c("1/8",0,0), X1 = c(0,"2/8","1/8"), X2 = c(0,"2/8","1/8"), X3 = c("1/8",0,0))

dXYvuoto[,2:5] <- "  "
@

\only<1>{
<<results='asis'>>=
addtorow <- list()
addtorow$pos <- list(  )
addtorow$pos[[1]] <- -1
addtorow$pos[[2]] <- 0
addtorow$command <- c("& \\multicolumn{4}{|c}{$X$ (nr. of heads)}\\\\"," $Y$ (nr. of switches) & 0 & 1 & 2 & 3 \\\\")

print(xtable(dXYvuoto,align="cc|cccc"),include.rownames = FALSE, add.to.row = addtorow, include.colnames = FALSE)
@
}

\only<2>{
<<results='asis'>>=
print(xtable(dXY,align="cc|cccc"),include.rownames = FALSE, add.to.row = addtorow, include.colnames = FALSE)
@
}


\footnotesize
The probability of two events happening together is called their conjoint probability. Example: $Pr(X = 1, Y = 2) = 1/8 = .125$

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{frame}{\textbf{Marginal probability distribution}}

\textbf{Marginal probability distributions} give us the probability of obtaining one variable outcome regardless of the value of the other variable(s).


<<>>=
dXY$Y_mar <- c("2/8","4/8","2/8") 
dXY$X1 <- factor(dXY$X1,levels = c("0","1/8","2/8","3/8") )
dXY$X2 <- factor(dXY$X2,levels = c("0","1/8","2/8","3/8") )
dXY <- rbind( dXY, c("$Y_{.}$", "1/8","3/8","3/8","1/8",NA) )
@

\pause
<<results='asis'>>=
addtorow$command <- c("& \\multicolumn{4}{|c}{$X$ (nr. of heads)} &  \\\\"," $Y$ (nr. of switches) & 0 & 1 & 2 & 3 & $X_{.}$  \\\\")

print(xtable(dXY,align="cc|cccc|c"),include.rownames = FALSE, add.to.row = addtorow, include.colnames = FALSE,sanitize.text.function = function(x){x}, hline.after = c(-1,0,3))
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Discrete and continuous marginal distributions}}

\begin{itemize}
 \item To compute the mariginal probability distribution of $X$, we sum $Pr(x, y)$ across all values of $y$
 \item When the $X$ and $Y$ variables are discrete, we will have: 
$$Pr(x) = \sum_y Pr(x, y)  \makebox[5cm][c]{(probability mass)}$$ 
 \item When the $X$ and $Y$ variables are continuous, we will have: 

$$p(x) = \int_y p(x, y) dy, \makebox[5cm][c]{(probability density)}$$
\end{itemize}

\footnotesize \vspace{.3cm}
Note. The process described above is called marginalizing over $Y$ or integrating out the variable $Y$.


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{frame}{\textbf{Conditional probability}}

\begin{itemize}
 \item We often want to know the probability of one event, given that we know another event is true
 \begin{itemize}
 \item What is the probability that you will pass the statistics exam given that you scored 20/30 on the first assignment?
 \item What is the probability of the observed data given that the \emph{Null Hypothesis} is true? What is the probability of the \emph{Null Hypothesis} being true given the observed data?!
\end{itemize} 
 \item Conditional probabilities are used to answer these kinds of questions
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Conditional probabilities: The idea}}

\begin{itemize}
 \item Let's go back to our example of three coin flips $\ldots$
 \item What is the probability that a sequence of three coin flips has 1 switch $(Y = 1)$ given that it has 1 head $(X = 1)$? \pause
$$Pr(Y = 1 | X = 1) = ?$$ (where “|” is read as: “given that”)
 \item Intuitively, we will must consider only the cases in which a single Head occurs. And then we will have to compare the probability of obtaining a Switch to the overall probability of having a single Head
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Conditional probabilities: The calculation}}

\begin{eqnarray*}
\begin{array}{rl}
Pr(Y = 1 | X = 1) & =  \frac{Pr(Y = 1, X = 1)}{Pr(Y = 0, X = 1)+Pr(Y = 1, X = 1)+Pr(Y = 2, X = 1)} \vspace{.3cm} \\ \pause
 & = \frac{2/8}{0 + 2/8 + 1/8} \vspace{.3cm} \\
 & = \frac{2/8}{3/8} \vspace{.3cm} \\
 & = \frac{2}{3} = \Sexpr{round(2/3,3)} 
\end{array}
\end{eqnarray*} \pause


\textbf{Question}: $Pr(X = 1 | Y = 1) = Pr(Y = 1 | X = 1)$?

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Conditional probabilities: The Formalization}}

\begin{itemize}
 \item When the $X$ and $Y$ variables are discrete, we will have: $$Pr(y|x) = \frac{Pr(y,x)}{\sum_y Pr(y,x)} = \frac{Pr(y,x)}{Pr(x)}$$
 \item When the $X$ and $Y$ variables are continuous, we will have: $$p(y|x) = \frac{p(y,x)}{\int_y p(y,x)dy} = \frac{p(y,x)}{p(x)}$$

\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{frame}{\textbf{Independent probability distributions}}

Two probability distribution, $p(x)$ and $p(y)$, are said to be independent if and only if: \\ \vspace{.3cm}
\begin{center}
$p(y | x) = p(y)$, for each $y$ and $x$ \\ \vspace{.3cm}
\end{center}

and equivalently \\ \vspace{.3cm}
\begin{center}
$p(x | y) = p(x)$, for each $x$ and $y$
\end{center}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Fundamental property of independent distributions}}

If and only if, the probability distribution of $X$ and $Y$ are indipendent, we will have:

\begin{center}
$p(x, y) = p(x)p(y)$, for each $y$ e $x$
\end{center}

\pause
\textbf{Exercises:}
\begin{enumerate}
 \item Demonstrate this simple property
 \item In our example of the three coin flips, are $X$
e $Y$ indipendent? Prove it.
\end{enumerate}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{frame}{\textbf{Before bayesian inference $\ldots$ remember that}}

\begin{itemize}
 \item \textbf{In general:} $Pr(x|y) \neq Pr(y|x)$ \\
If someone smiles at you, what is the probability that they love you? \\
If someone loves you, what is the probability that they will smile at you? 
 \item \textbf{There is no temporal order in conditional probabilities} \\
 When we say “the probability of $x$ given $y$” we do not mean that $y$ has already happened and $x$ has yet to happen. All we mean is that we are restricting our calculations of probability to a particular subset of possible events. 
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Bonus: Going Bayes}}

\small \vspace{-.3cm}
\begin{itemize}
 \item We know that: 
$$Pr(y|x) = \frac{Pr(y,x)}{Pr(x)} \makebox[.5cm][l]{,} Pr(x|y) = \frac{Pr(y,x)}{Pr(y)}$$ \pause
 
 \item Obtain $Pr(y, x)$ from both equations and equalize the results: $$Pr(y|x) Pr(x) = Pr(x|y)Pr(y)$$ \pause

 \item Divide both sides of the equation by $Pr(x)$: $$Pr(y|x) = \frac{Pr(x|y)Pr(y)}{ Pr(x)}$$ 
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Bonus: Going Bayes}}
 
\begin{itemize}
 \item Now, remembering that $Pr(x) = \sum_y Pr(x, y)$, and knowing that $Pr(x, y) = Pr(x|y)Pr(y)$ \pause
 $\ldots$ here we have the \\ \textbf{Bayes Theorem}: $$Pr(y|x) = \frac{Pr(x|y)Pr(y)}{\sum_y Pr(x, y)}$$

\end{itemize}
\end{frame}


%%%%%%%%%%%%
\section{The sampling idea}
\begin{frame}[plain]

\begin{beamerboxesrounded}[upper=title, lower=structure, shadow=true]
    {\center{\Large{\textbf{The sampling idea}}}}
\end{beamerboxesrounded}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{ }}

\begin{itemize}
 \item Una importante caratteristica utile per l'approccio Baiesiano è quella del campionamento.
 \item Ovvero, se non conosciamo la forma analitica della \emph{posterior}, la approssimiamo per campionamento. 
\end{itemize} 

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Example}}

Supponiamo di voler approssimare una distribuzione ignota per campionamento; sappiamo solo che è normale ma non conosciamo i parametri $\mu$ e $\sigma$.



\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\textbf{Example}}

<<>>=
mu <- 5; sigma <- 2
Omega <- rnorm( 1e7, mu, sigma )
#curve(dnorm(x,mu,sigma),0,10)
@

\only<1>{
<<fig.height=3,dev='pdf'>>=
library(ggplot2)
set.seed(1)
y <- sample( Omega, 1 )
myData <- data.frame(y=y)
ggplot( myData ) + theme_minimal()
@

}

\only<2>{

<<echo=TRUE>>=
y <- sample( Omega, 1 )
@

<<fig.height=2>>=
myData <- data.frame(y=y)
ggplot( myData, aes(x=y,y=0) ) + theme_bw() + geom_point(color="red", size = 3) + scale_y_continuous(limits = c(0,1),breaks = NULL) + xlab("") + ylab("")
@

}

\only<3>{

<<echo=TRUE>>=
y <- sample( Omega, 10 )
@

<<fig.height=2,message=FALSE>>=
myData <- data.frame(y=y)
ggplot( myData, aes(x=y) ) + theme_bw() + geom_density() + 
  geom_point(color="red", fill = "red",aes(x=y,y=0)) + scale_y_continuous( breaks = NULL ) + xlab("") + ylab("") 
@

}

\only<4>{

<<echo=TRUE>>=
y <- sample( Omega, 100 )
@

<<fig.height=2,message=FALSE>>=
myData <- data.frame(y=y)
ggplot( myData, aes(x=y) ) + theme_bw() + geom_density() + 
  geom_rug(color="red") + scale_y_continuous( breaks = NULL ) + xlab("") + ylab("") 
@

}

\only<5>{

<<echo=TRUE>>=
y <- sample( Omega, 1000 )
@

<<fig.height=2,message=FALSE>>=
myData <- data.frame(y=y)
ggplot( myData, aes(x=y) ) + theme_bw() + geom_density() + 
  geom_rug(color="red") + scale_y_continuous( breaks = NULL ) + xlab("") + ylab("") 
@


}

\only<6>{
<<echo=TRUE>>=
y <- sample( Omega, 10000 )
@

<<fig.height=2,message=FALSE>>=
xx <- quantile(Omega,probs = .997)
yy <- dnorm(6,mu,sigma)

GG <- ggplot( myData, aes(x=y) ) + theme_bw() + geom_density() + 
    geom_rug( color = "red" )+ scale_y_continuous( breaks = NULL ) + xlab("") + ylab("")

GG + annotate("text", x=xx, y=yy, label ="$\\mathcal{N}(?,?)$")

@

}

\only<7>{
<<echo=TRUE>>=
y <- sample( Omega, 10000 )
@

<<fig.height=2,message=FALSE>>=
GG + stat_function( fun = dnorm, args = list(mean=mu,sd=sigma), color = "blue", lwd = 1.5 ) + annotate("text", x=xx, y=yy, label =paste0("$\\mathcal{N}(",mu,", ",sigma,")$"))
@

}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{}
\begin{frame}[plain]{\textbf{Main references}}

\begin{thebibliography}{10}
 \bibitem{K} Kruschke, J., K. (2014).  
\emph{Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan}. Academic Press/Elsevier Science

 \bibitem{L} Lambert, B. (2018). \emph{A Student's Guide to Bayesian Statistics}. London, UK: SAGE.

 \bibitem{Mc} McElreath, R. (2016). 
\emph{Statistical Rethinking: A Bayesian Course with Examples in R and Stan}. CRC Press.
\end{thebibliography}


\end{frame}
%%%%%%%%%%%%%%%%%%%% FINE DOCUMENTO
\begin{frame}[plain,fragile]{\textbf{Used R packages}}

<<results='markup',message=FALSE>>=
options(width = 80)
library(report)
SI <- report(sessionInfo())
PACK <- attr(SI,"table")$Package
REF <- attr(SI,"table")$Reference
REF <- gsub("#","", gsub("&","\\\\&",gsub("<","",gsub(">","",gsub("_","\\\\_",REF)))))

REF <- REF[!grepl("kandinsky",PACK)] 
#REF <- REF[!grepl("R Core Team",REF)]

PACK <- PACK[!grepl("kandinsky",PACK)]
@

\scriptsize
\begin{itemize}
<<results='asis'>>=

L <- gregexpr("\\).",REF) 
fine <- unlist(lapply(L, function(x){
  return(x[1]+1)
}))

for (k in 1:length(PACK)) {
  cat(paste0("\\item \\texttt{",PACK[k],"}. "))
  cat(substr(REF[k],1,fine[k]),"\n")
}
@
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{.}
\begin{frame}[plain]
  \frametitle{}

\begin{flushright}
  \scalebox{.013}{\includegraphics{img/logo_psicostat.png}}
\end{flushright}


\vspace{1.5cm}
\begin{center}
\texttt{massimiliano.pastore@unipd.it}
\url{https://psicostat.dpss.psy.unipd.it/}
\end{center}

\vspace{1cm}
\begin{flushright}
  \scalebox{.25}{\includegraphics{img/loghi2017.png}}
\end{flushright}
\end{frame}

\end{document}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{FINO QUI}
<<finale>>=
# KUtils::pulizia(paste(main,"knitr/",sep=""), c(".Rnw",".bib"))
opts_chunk$set(eval=FALSE,include = FALSE)
@
\end{document}



