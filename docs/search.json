[
  {
    "objectID": "exercises/phd-stress.html",
    "href": "exercises/phd-stress.html",
    "title": "PhD Stress Level Dataset and Exercises",
    "section": "",
    "text": "Dataset Description\nThis dataset contains information on PhD students’ stress levels (scale 0 to 100) along with several predictors:\n\nCoffee: Number of coffee cups consumed per day (integer)\n\nSpritz: Number of spritz (aperitif) consumed per week (integer)\n\nStatistical Knowledge: Self-assessed score from 0 to 10\n\nSoftware Used: Statistical software used (R, SPSS, Excel)\n\nYou can download the dataset here\n\n\nExercises\nThese exercises will help you explore the dataset, understand relationships, and fit a linear regression model.\n\n1. Load and Explore the Data\n\nLoad the dataset.\n\nFix problems in the dataset, impute missing numerical values with the mean and missing categorical variables with a random value from the available values.\nCalculate means, medians, standard deviations, and ranges for all numeric variables.\n\nExamine the distribution of the categorical variable software_used with counts and proportions.\n\n\n\n2. Visualize the Data\n\nPlot histograms or density plots for the numeric variables (stress, coffee, spritz, statistical knowledge).\n\nCreate boxplots of stress by software_used.\n\nCreate scatterplots:\n\nStress vs Coffee\n\nStress vs Spritz (color points by software_used)\n\nStress vs Statistical Knowledge\n\n\n\n\n3. Check Relationships and Correlations\n\nCompute correlation coefficients between numeric variables.\n\nComment on the strength and direction of relationships between stress and each predictor.\n\n\n\n4. Fit a Linear Regression Model\n\nFit a linear regression model predicting stress from coffee, spritz, statistical knowledge, software_used, and the interaction between spritz and software\nExamine the model summary and interpret the coefficients, especially the interaction terms"
  },
  {
    "objectID": "exercises/well-being.html",
    "href": "exercises/well-being.html",
    "title": "Well-being and Psychological Predictors",
    "section": "",
    "text": "This dataset is about well-being scores (range: 10–100) with a series of psychological, behavioral, and demographic predictors.\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\nage\nContinuous\nAge in years\n\n\ngender\nCategorical\nMale, Female, or Other\n\n\nincome\nContinuous\nAnnual income in €\n\n\nexercise_freq\nCount\nTimes exercising per week\n\n\nsocial_support\nContinuous\nPerceived social support (0–10 scale)\n\n\ntherapy\nCategorical\nWhether the individual is in therapy\n\n\nwell_being\nContinuous\nSelf-reported well-being (10 to 100)\n\n\n\n\n\n\nWe generated the data based on the following hypotheses:\n\nH1: Higher income is associated with higher well-being.\nH2: Greater social support is associated with higher well-being.\nH3: More frequent exercise is associated with higher well-being.\nH4: Individuals in therapy report higher well-being.\nH5: Age is negatively associated with well-being.\nH6: Therapy is especially beneficial for individuals with low social support (interaction).\nH7: The positive effect of exercise on well-being increases with income (interaction).\nH8: Gender has no effect on well-being (test of a null effect).\n\n\n\n\n       age gender   income exercise_freq social_support therapy well_being\n1 48.70958   Male 16961.79             1       6.301848      No   24.77800\n2 29.35302 Female 27490.86             6       5.420772      No   49.12772\n3 38.63128   Male 31710.07             0       4.310958      No   35.87966\n4 41.32863 Female 25965.33             6       5.050138      No   43.59889\n5 39.04268   Male 31046.59             2       9.075336      No   52.10580\n6 33.93875   Male 26811.19             4       0.000000     Yes   50.28308\n\n\nYou can find the dataset here"
  },
  {
    "objectID": "exercises/well-being.html#dataset-description",
    "href": "exercises/well-being.html#dataset-description",
    "title": "Well-being and Psychological Predictors",
    "section": "",
    "text": "This dataset is about well-being scores (range: 10–100) with a series of psychological, behavioral, and demographic predictors.\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\nage\nContinuous\nAge in years\n\n\ngender\nCategorical\nMale, Female, or Other\n\n\nincome\nContinuous\nAnnual income in €\n\n\nexercise_freq\nCount\nTimes exercising per week\n\n\nsocial_support\nContinuous\nPerceived social support (0–10 scale)\n\n\ntherapy\nCategorical\nWhether the individual is in therapy\n\n\nwell_being\nContinuous\nSelf-reported well-being (10 to 100)\n\n\n\n\n\n\nWe generated the data based on the following hypotheses:\n\nH1: Higher income is associated with higher well-being.\nH2: Greater social support is associated with higher well-being.\nH3: More frequent exercise is associated with higher well-being.\nH4: Individuals in therapy report higher well-being.\nH5: Age is negatively associated with well-being.\nH6: Therapy is especially beneficial for individuals with low social support (interaction).\nH7: The positive effect of exercise on well-being increases with income (interaction).\nH8: Gender has no effect on well-being (test of a null effect).\n\n\n\n\n       age gender   income exercise_freq social_support therapy well_being\n1 48.70958   Male 16961.79             1       6.301848      No   24.77800\n2 29.35302 Female 27490.86             6       5.420772      No   49.12772\n3 38.63128   Male 31710.07             0       4.310958      No   35.87966\n4 41.32863 Female 25965.33             6       5.050138      No   43.59889\n5 39.04268   Male 31046.59             2       9.075336      No   52.10580\n6 33.93875   Male 26811.19             4       0.000000     Yes   50.28308\n\n\nYou can find the dataset here"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "Introduction to Bayesian Statistics",
    "section": "Slides",
    "text": "Slides\n\nParte 1 -  - Rnw"
  },
  {
    "objectID": "index.html#other-materials",
    "href": "index.html#other-materials",
    "title": "Introduction to Bayesian Statistics",
    "section": "Other Materials",
    "text": "Other Materials\n\nParte 1 - Introduzione a R1\nParte 5 - Generalized Linear Models2"
  },
  {
    "objectID": "index.html#exercises",
    "href": "index.html#exercises",
    "title": "Introduction to Bayesian Statistics",
    "section": "Exercises",
    "text": "Exercises\n\nEsercises - \nWell-being dataset\nPhD stress dataset"
  },
  {
    "objectID": "index.html#datasets",
    "href": "index.html#datasets",
    "title": "Introduction to Bayesian Statistics",
    "section": "Datasets",
    "text": "Datasets\nYou can find all the datasets here."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction to Bayesian Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is an intensive course about R, we will see some useful elements↩︎\nThis is an intensive PhD level course about GLMs↩︎"
  },
  {
    "objectID": "slides/bayesian-glm.html#almost-everything-is-a-linear-model",
    "href": "slides/bayesian-glm.html#almost-everything-is-a-linear-model",
    "title": "Generalized Linear Models with brms",
    "section": "(almost) everything is a linear model",
    "text": "(almost) everything is a linear model\nMost of the statistical analysis that you usually perfom, is essentially a linear model.\n\nThe t-test is a linear model where a numerical variable y is predicted by a factor with two levels x\nThe one-way anova is a linear model where a numerical variable y is predicted by one factor with more than two levels x\nThe correlation is a linear model where a numerical variable y is predicted by another numerical variable x\nThe ancova is a linear model where a numerical variable y is predicted by a numerical variable x and a factor with two levels g\n…"
  },
  {
    "objectID": "slides/bayesian-glm.html#what-is-a-linear-model",
    "href": "slides/bayesian-glm.html#what-is-a-linear-model",
    "title": "Generalized Linear Models with brms",
    "section": "What is a linear model?",
    "text": "What is a linear model?\nLet’s start with a single variable y. We assume that the variable comes from a Normal distribution:"
  },
  {
    "objectID": "slides/bayesian-glm.html#what-is-a-linear-model-1",
    "href": "slides/bayesian-glm.html#what-is-a-linear-model-1",
    "title": "Generalized Linear Models with brms",
    "section": "What is a linear model?",
    "text": "What is a linear model?\nWhat we can do with this variable? We can estimate the parameters that define the Normal distribution thus \\(\\mu\\) (the mean) and \\(\\sigma\\) (the standard deviation).\n\nmean(y)\n#&gt; [1] 100\nsd(y)\n#&gt; [1] 50"
  },
  {
    "objectID": "slides/bayesian-glm.html#what-is-a-linear-model-2",
    "href": "slides/bayesian-glm.html#what-is-a-linear-model-2",
    "title": "Generalized Linear Models with brms",
    "section": "What is a linear model?",
    "text": "What is a linear model?\nUsing a linear model we can just fit a model without predictors, also known as intercept-only model.\n\nfit &lt;- glm(y ~ 1, family = gaussian(link = \"identity\"))\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ 1, family = gaussian(link = \"identity\"))\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)      100          5      20   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for gaussian family taken to be 2500)\n#&gt; \n#&gt;     Null deviance: 247500  on 99  degrees of freedom\n#&gt; Residual deviance: 247500  on 99  degrees of freedom\n#&gt; AIC: 1069.2\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "slides/bayesian-glm.html#what-is-a-linear-model-3",
    "href": "slides/bayesian-glm.html#what-is-a-linear-model-3",
    "title": "Generalized Linear Models with brms",
    "section": "What is a linear model?",
    "text": "What is a linear model?\nI am using glm because I want to estimate parameters using Maximul Likelihood, but the results are the same as using lm.\nBasically we estimated the mean (Intercept) and the standard deviation Dispersion, just take the square root thus 50.\nWhat we are doing is essentially finding the \\(\\mu\\) and \\(\\sigma\\) that maximised the log-likelihood of the model fixing the observed data."
  },
  {
    "objectID": "slides/bayesian-glm.html#what-is-a-linear-model-4",
    "href": "slides/bayesian-glm.html#what-is-a-linear-model-4",
    "title": "Generalized Linear Models with brms",
    "section": "What is a linear model?",
    "text": "What is a linear model?"
  },
  {
    "objectID": "slides/bayesian-glm.html#what-is-a-linear-model-5",
    "href": "slides/bayesian-glm.html#what-is-a-linear-model-5",
    "title": "Generalized Linear Models with brms",
    "section": "What is a linear model?",
    "text": "What is a linear model?\nAnd assuming that we know \\(\\sigma\\) (thus fixing it at 50):"
  },
  {
    "objectID": "slides/bayesian-glm.html#what-is-a-linear-model-6",
    "href": "slides/bayesian-glm.html#what-is-a-linear-model-6",
    "title": "Generalized Linear Models with brms",
    "section": "What is a linear model?",
    "text": "What is a linear model?\nThus, with the estimates of glm, we have this model fitted on the data:"
  },
  {
    "objectID": "slides/bayesian-glm.html#including-a-predictor",
    "href": "slides/bayesian-glm.html#including-a-predictor",
    "title": "Generalized Linear Models with brms",
    "section": "Including a predictor",
    "text": "Including a predictor\nWhen we include a predictor, we are actually try to explain the variability of y using a variable x. For example, this is an hypothetical relationship:\n\n\n\n\n\n\n\n\n\nSeems that there is a positive (linear) relationship between x and y. We can try to improve the previous model by adding the predictor:\n\nfit &lt;- glm(y ~ x, family = gaussian(link = \"identity\"))\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ x, family = gaussian(link = \"identity\"))\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  9.99660    0.09248 108.093   &lt;2e-16 ***\n#&gt; x            0.46761    0.09768   4.787    6e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for gaussian family taken to be 0.8550155)\n#&gt; \n#&gt;     Null deviance: 103.385  on 99  degrees of freedom\n#&gt; Residual deviance:  83.792  on 98  degrees of freedom\n#&gt; AIC: 272.1\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "slides/bayesian-glm.html#assumptions-of-the-linear-model",
    "href": "slides/bayesian-glm.html#assumptions-of-the-linear-model",
    "title": "Generalized Linear Models with brms",
    "section": "Assumptions of the linear model",
    "text": "Assumptions of the linear model\nMore practicaly, we are saying that the model allows for varying the mean i.e., each x value can be associated with a different \\(\\mu\\) but with a fixed (and estimated) \\(\\sigma\\)."
  },
  {
    "objectID": "slides/bayesian-glm.html#recipe-for-a-glm",
    "href": "slides/bayesian-glm.html#recipe-for-a-glm",
    "title": "Generalized Linear Models with brms",
    "section": "Recipe for a GLM",
    "text": "Recipe for a GLM\n\nRandom Component\nSystematic Component\nLink Function"
  },
  {
    "objectID": "slides/bayesian-glm.html#random-component",
    "href": "slides/bayesian-glm.html#random-component",
    "title": "Generalized Linear Models with brms",
    "section": "Random Component",
    "text": "Random Component\nThe random component of a GLM identify the response variable \\(y\\) coming from a certain probability distribution."
  },
  {
    "objectID": "slides/bayesian-glm.html#random-component-1",
    "href": "slides/bayesian-glm.html#random-component-1",
    "title": "Generalized Linear Models with brms",
    "section": "Random Component",
    "text": "Random Component\n\nIn practice, by definition the GLM is a model where the random component is a distribution of the Exponential Family. For example the Gaussian distribution, the Gamma distribution or the Binomial are part of the Exponential Family.\nThese distribution can be described using a location parameter (e.g., the mean) and a scale parameter (e.g., the variance).\nThe distributions are defined by parameters (e.g., \\(\\mu\\) and \\(\\sigma\\) for the Gaussian or \\(\\lambda\\) for the Poisson). The location (or mean) can be directly one of the parameter or a combination of parameters."
  },
  {
    "objectID": "slides/bayesian-glm.html#random-component-poisson-example",
    "href": "slides/bayesian-glm.html#random-component-poisson-example",
    "title": "Generalized Linear Models with brms",
    "section": "Random Component, Poisson example",
    "text": "Random Component, Poisson example\nFor example, the Poisson distribution is defined as:\n\\[\nf(k,\\lambda) = Pr(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\]\nWhere \\(k\\) is the number of events and \\(\\lambda\\) (the only parameter) is the rate."
  },
  {
    "objectID": "slides/bayesian-glm.html#random-component-poisson-example-1",
    "href": "slides/bayesian-glm.html#random-component-poisson-example-1",
    "title": "Generalized Linear Models with brms",
    "section": "Random Component, Poisson example",
    "text": "Random Component, Poisson example\nThe mean or location of the Poisson is \\(\\lambda\\) and also the scale or variance is \\(\\lambda\\). Compared to the Gaussian, there are no two parameters."
  },
  {
    "objectID": "slides/bayesian-glm.html#random-component-2",
    "href": "slides/bayesian-glm.html#random-component-2",
    "title": "Generalized Linear Models with brms",
    "section": "Random Component",
    "text": "Random Component\nTo sum-up, the random component represents the assumption about the nature of our response variable. With GLM we want to include predictors to explain systematic changes of the mean (but also the scale/variance) of the random component.\nAssuming a Gaussian distribution, we try to explain how the mean of the Gaussian distribution change according to our predictors. For the Poisson, we include predictors on the \\(\\lambda\\) parameters for example.\nThe Random Component is called random, beacause it determines how the error term \\(\\epsilon\\) of our model is distributed."
  },
  {
    "objectID": "slides/bayesian-glm.html#systematic-component",
    "href": "slides/bayesian-glm.html#systematic-component",
    "title": "Generalized Linear Models with brms",
    "section": "Systematic Component",
    "text": "Systematic Component\nThe systematic component of a GLM is the combination of predictors (i.e., independent variables) that we want to include in the model.\nThe systematic component is also called linear predictor \\(\\eta\\) and is usually written in equation terms as: \\[\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}\n\\]\nNote that I am omitting the \\(+ \\epsilon_i\\) that you usually find at the end because this is the combination of predictors without errors."
  },
  {
    "objectID": "slides/bayesian-glm.html#systematic-component-an-example",
    "href": "slides/bayesian-glm.html#systematic-component-an-example",
    "title": "Generalized Linear Models with brms",
    "section": "Systematic Component, an example",
    "text": "Systematic Component, an example\nAssuming that we have two groups and we want to see if there are differences in a depression score. This is a t-test, or better a linear model, or better a GLM.\nIgnoring the random component, we can have a systematic component written in this way:\n\\[\n\\eta_i = \\beta_0 + \\beta_1{\\mbox{group}_i}\n\\]\nAssuming that the group is dummy-coded, \\(\\beta_0\\) is the mean of the first group and \\(\\beta_1\\) is the difference between the two groups. In other terms, these are the true or estimated values without the error (i.e., the random component)."
  },
  {
    "objectID": "slides/bayesian-glm.html#systematic-component-an-example-1",
    "href": "slides/bayesian-glm.html#systematic-component-an-example-1",
    "title": "Generalized Linear Models with brms",
    "section": "Systematic Component, an example",
    "text": "Systematic Component, an example\nAnother example, assuming we have the same depression score and we want to predict it with an anxiety score. The blue line is the true/estimated regression line where \\(\\eta_i\\) is the expected value for the observation \\(x_i\\). The red segments are the errors or residuals i.e., the random component."
  },
  {
    "objectID": "slides/bayesian-glm.html#systematic-component-1",
    "href": "slides/bayesian-glm.html#systematic-component-1",
    "title": "Generalized Linear Models with brms",
    "section": "Systematic Component",
    "text": "Systematic Component\nTo sum-up, the systematic component is the combination of predictors that are used to predict the mean of the distribution that is used as random component. The errors part of the model is distributed as the random component."
  },
  {
    "objectID": "slides/bayesian-glm.html#link-function",
    "href": "slides/bayesian-glm.html#link-function",
    "title": "Generalized Linear Models with brms",
    "section": "Link Function",
    "text": "Link Function\nThe final element is the link function. The idea is that we need a way to connect the systematic component \\(\\eta\\) to the random component mean \\(\\mu\\).\nThe link function \\(g(\\mu)\\) is an invertible function that connects the mean \\(\\mu\\) of the random component with the linear combination of predictors.\nThus \\(\\eta_i = g(\\mu_i)\\) and \\(\\mu_i = g(\\eta_i)^{-1}\\). The systematic component is not affected by \\(g()\\) while the relationship between \\(\\mu\\) and \\(\\eta\\) changes using different link functions.\n\\[\ng(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}\n\\]\n\\[\n\\mu_i = g(\\eta_i)^{-1} = \\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}\n\\]"
  },
  {
    "objectID": "slides/bayesian-glm.html#link-function-1",
    "href": "slides/bayesian-glm.html#link-function-1",
    "title": "Generalized Linear Models with brms",
    "section": "Link function",
    "text": "Link function\nThe simplest link function is the identity link where \\(g(\\mu) = \\mu\\) and correspond to the standard linear model. In fact, the linear regression is just a GLM with a Gaussian random component and the identity link function.\n\n\n\n\n\n\nMain distributions and link functions\n\n\nFamily\nLink\nRange\n\n\n\n\ngaussian\nidentity\n\\[(-\\infty,+\\infty)\\]\n\n\ngamma\nlog\n\\[(0,+\\infty)\\]\n\n\nbinomial\nlogit\n\\[\\frac{0, 1, ..., n_{i}}{n_{i}}\\]\n\n\nbinomial\nprobit\n\\[\\frac{0, 1, ..., n_{i}}{n_{i}}\\]\n\n\npoisson\nlog\n\\[0, 1, 2, ...\\]"
  },
  {
    "objectID": "slides/bayesian-glm.html#gaussian-glm",
    "href": "slides/bayesian-glm.html#gaussian-glm",
    "title": "Generalized Linear Models with brms",
    "section": "Gaussian GLM",
    "text": "Gaussian GLM\nThus remember that when you do a lm or lmer you are actually doing a GLM with a Gaussian random component and an identity link function. You are including predictors (systematic component) explaining changes in the mean of the Gaussian distribution.\n\n\n\nlm(y ~ x)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            x  \n#&gt;      0.2368       0.3688\n\n\n\n\nglm(y ~ x, family = gaussian(link = \"identity\"))\n\n#&gt; \n#&gt; Call:  glm(formula = y ~ x, family = gaussian(link = \"identity\"))\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            x  \n#&gt;      0.2368       0.3688  \n#&gt; \n#&gt; Degrees of Freedom: 99 Total (i.e. Null);  98 Residual\n#&gt; Null Deviance:       109.6 \n#&gt; Residual Deviance: 96.23     AIC: 285.9"
  },
  {
    "objectID": "slides/bayesian-glm.html#gaussian-glm-a-simple-simulation",
    "href": "slides/bayesian-glm.html#gaussian-glm-a-simple-simulation",
    "title": "Generalized Linear Models with brms",
    "section": "Gaussian GLM, a simple simulation",
    "text": "Gaussian GLM, a simple simulation\nWe can understand the GLM recipe trying to simulate a simple model. Let’s simulate a relationship between two numerical variables (like the depression and anxiety example).\n\nN &lt;- 20\nanxiety &lt;- rnorm(N, 0, 1) # anxiety scores\nb0 &lt;- 0.3 # intercept, depression when anxiety = 0\nb1 &lt;- 0.5 # increase in depression for 1 increase in anxiety\n\n# systematic component\neta &lt;- b0 + b1 * anxiety\n\ndat &lt;- data.frame(anxiety, b0, b1, eta)\nhead(dat)\n\n#&gt;      anxiety  b0  b1         eta\n#&gt; 1  0.4892000 0.3 0.5  0.54460001\n#&gt; 2 -1.5912630 0.3 0.5 -0.49563148\n#&gt; 3 -0.4770213 0.3 0.5  0.06148934\n#&gt; 4 -1.8916850 0.3 0.5 -0.64584248\n#&gt; 5 -0.4024015 0.3 0.5  0.09879926\n#&gt; 6  0.7335647 0.3 0.5  0.66678234"
  },
  {
    "objectID": "slides/bayesian-glm.html#gaussian-glm-a-simple-simulation-1",
    "href": "slides/bayesian-glm.html#gaussian-glm-a-simple-simulation-1",
    "title": "Generalized Linear Models with brms",
    "section": "Gaussian GLM, a simple simulation",
    "text": "Gaussian GLM, a simple simulation\neta is the linear predictor (without errors):\n\nThus the expected value of a person with \\(\\mbox{anxiety} = -1\\) is \\(\\beta_0 + \\beta_1\\times(-1)\\) thus -0.2."
  },
  {
    "objectID": "slides/bayesian-glm.html#gaussian-glm-a-simple-simulation-2",
    "href": "slides/bayesian-glm.html#gaussian-glm-a-simple-simulation-2",
    "title": "Generalized Linear Models with brms",
    "section": "Gaussian GLM, a simple simulation",
    "text": "Gaussian GLM, a simple simulation\nNow, for a realistic simulation we need some random errors. The random component here is a Gaussian distribution thus each observed (or simulated) value is the systematic component plus the random error \\(\\mbox{depression}_i = \\eta_i + \\epsilon_i\\).\nThe errors (or residuals) are assumed to be normally distributed with \\(\\mu = 0\\) and variance \\(\\sigma^2_{\\epsilon}\\) (the residual standard deviation).\n\nsigma &lt;- 1 # residual standard deviation\nerror &lt;- rnorm(N, 0, sigma)\ndepression &lt;- eta + error # b0 + b1 * anxiety + error"
  },
  {
    "objectID": "slides/bayesian-glm.html#gaussian-glm-a-simple-simulation-3",
    "href": "slides/bayesian-glm.html#gaussian-glm-a-simple-simulation-3",
    "title": "Generalized Linear Models with brms",
    "section": "Gaussian GLM, a simple simulation",
    "text": "Gaussian GLM, a simple simulation\nThis is the simulated dataset. The blue line is the linear predictor and the red segments are the Gaussian residuals."
  },
  {
    "objectID": "slides/bayesian-glm.html#gaussian-glm-a-simple-simulation-4",
    "href": "slides/bayesian-glm.html#gaussian-glm-a-simple-simulation-4",
    "title": "Generalized Linear Models with brms",
    "section": "Gaussian GLM, a simple simulation",
    "text": "Gaussian GLM, a simple simulation\nIf we plot the red segments we have roughly a Gaussian distribution. This is the assumption of the GLM with a Gaussian random component."
  },
  {
    "objectID": "slides/bayesian-glm.html#what-about-the-link-function",
    "href": "slides/bayesian-glm.html#what-about-the-link-function",
    "title": "Generalized Linear Models with brms",
    "section": "What about the link function?",
    "text": "What about the link function?\nThe link function for the Gaussian GLM is by default the identity. Identity means that \\(\\eta_i = \\mu_i\\), thus there is no transformation. Within each distribution object in R there is the link function and the inverse:\n\n# this is the family (or random component) and the link function. doing a lm() is like glm(family = gaussian(link = \"identity\"))\nfam &lt;- gaussian(link = \"identity\")\nfam$linkfun # link function specifed above\n\n#&gt; function (mu) \n#&gt; mu\n#&gt; &lt;environment: namespace:stats&gt;\n\nfam$linkinv # inverse link function\n\n#&gt; function (eta) \n#&gt; eta\n#&gt; &lt;environment: namespace:stats&gt;"
  },
  {
    "objectID": "slides/bayesian-glm.html#what-about-the-link-function-1",
    "href": "slides/bayesian-glm.html#what-about-the-link-function-1",
    "title": "Generalized Linear Models with brms",
    "section": "What about the link function?",
    "text": "What about the link function?\nWith the identity, the link function has no effect.\n\nmu &lt;- fam$linkinv(b0 + b1 * anxiety)\nhead(mu)\n\n#&gt; [1]  0.54460001 -0.49563148  0.06148934 -0.64584248  0.09879926  0.66678234\n\nhead(fam$linkfun(mu))\n\n#&gt; [1]  0.54460001 -0.49563148  0.06148934 -0.64584248  0.09879926  0.66678234\n\n\nBut with other GLMs, (e.g., logistic regression) the link function is the core element."
  },
  {
    "objectID": "slides/bayesian-glm.html#gaussian-glm-a-simple-simulation-5",
    "href": "slides/bayesian-glm.html#gaussian-glm-a-simple-simulation-5",
    "title": "Generalized Linear Models with brms",
    "section": "Gaussian GLM, a simple simulation",
    "text": "Gaussian GLM, a simple simulation\nA more compact (and useful) way to simulate the data is:\n\ndepression &lt;- rnorm(N, mean = fam$linkinv(b0 + b1 * anxiety), sd = sigma)\n\nIn this way is more clear that we are generating data from a normal distribution with fixed \\(\\sigma^2_{\\epsilon}\\) and we are modeling the mean."
  },
  {
    "objectID": "slides/bayesian-glm.html#parameters-intepretation-1",
    "href": "slides/bayesian-glm.html#parameters-intepretation-1",
    "title": "Generalized Linear Models with brms",
    "section": "Parameters intepretation",
    "text": "Parameters intepretation\nLet’s make a more complex example with a Gaussian GLM with more than one predictor. We have a dataset with 150 observations and some variables.\n\n\n#&gt;   depression age group    anxiety\n#&gt; 1 -0.2173456  23    g1  0.0022093\n#&gt; 2  0.3273544  40    g2 -0.1100926\n#&gt; 3  1.0405812  24    g1  0.4261357\n#&gt; 4  2.8139121  33    g2  1.8146185\n#&gt; 5  0.7347837  30    g1 -0.1817395\n#&gt; 6  0.6209260  26    g2 -1.2488724\n\n\nWe want to predict the depression with anxiety, group and age."
  },
  {
    "objectID": "slides/bayesian-glm.html#parameters-intepretation-2",
    "href": "slides/bayesian-glm.html#parameters-intepretation-2",
    "title": "Generalized Linear Models with brms",
    "section": "Parameters intepretation",
    "text": "Parameters intepretation\nLet’s fit the model (here using lm but is a GLM!):\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = depression ~ anxiety + group + age, data = dat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.0837 -0.6937 -0.1653  0.5869  3.1663 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -0.229923   0.326055  -0.705  0.48183    \n#&gt; anxiety      0.526979   0.081160   6.493 1.23e-09 ***\n#&gt; groupg2      0.367025   0.165569   2.217  0.02819 *  \n#&gt; age          0.024005   0.009075   2.645  0.00906 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.013 on 146 degrees of freedom\n#&gt; Multiple R-squared:  0.2574, Adjusted R-squared:  0.2421 \n#&gt; F-statistic: 16.87 on 3 and 146 DF,  p-value: 1.844e-09\n\n\nHow do you intepret the output? and the model parameters?"
  },
  {
    "objectID": "slides/bayesian-glm.html#bayesian-vs-frequentists-glm",
    "href": "slides/bayesian-glm.html#bayesian-vs-frequentists-glm",
    "title": "Generalized Linear Models with brms",
    "section": "Bayesian vs Frequentists GLM",
    "text": "Bayesian vs Frequentists GLM\nWhat about the Bayesian version of the previous model? Actually the main difference is that we need to include the priors to obtain posterior distributions about model parameters. The likelihood part is extactly the same as non-bayesian models."
  },
  {
    "objectID": "slides/bayesian-glm.html#brms",
    "href": "slides/bayesian-glm.html#brms",
    "title": "Generalized Linear Models with brms",
    "section": "brms",
    "text": "brms\nThere are several R packages for estimating Bayesian GLMs. The most complete is called brms.\nThere are also other options such as rstanarm. rstanarm is faster but less flexible. brms include all GLMs (and also other models such as meta-analysis, multivariate, etc.) but is slower and requires more knowledge.\nThe syntax is the same as lm or glm and also lme4 if you want to include random-effects."
  },
  {
    "objectID": "slides/bayesian-glm.html#brms-1",
    "href": "slides/bayesian-glm.html#brms-1",
    "title": "Generalized Linear Models with brms",
    "section": "brms",
    "text": "brms\nLet’s start with a simple model, predicting the depression with the group. Thus essentially a t-test:\n\nfit_group &lt;- brm(depression ~ group, \n                 data = dat, \n                 family = gaussian(link = \"identity\"), \n                 file = here(\"slides/objects/fit_group.rds\"))"
  },
  {
    "objectID": "slides/bayesian-glm.html#brms-2",
    "href": "slides/bayesian-glm.html#brms-2",
    "title": "Generalized Linear Models with brms",
    "section": "brms",
    "text": "brms\n\nsummary(fit_group)\n\n#&gt;  Family: gaussian \n#&gt;   Links: mu = identity; sigma = identity \n#&gt; Formula: depression ~ group \n#&gt;    Data: dat (Number of observations: 150) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Regression Coefficients:\n#&gt;           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept     0.56      0.13     0.31     0.82 1.00     3601     2924\n#&gt; groupg2       0.44      0.18     0.08     0.80 1.00     3456     2832\n#&gt; \n#&gt; Further Distributional Parameters:\n#&gt;       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; sigma     1.11      0.07     1.00     1.26 1.00     3865     2502\n#&gt; \n#&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential\n#&gt; scale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides/bayesian-glm.html#brms-vs-lm",
    "href": "slides/bayesian-glm.html#brms-vs-lm",
    "title": "Generalized Linear Models with brms",
    "section": "brms vs lm",
    "text": "brms vs lm\nFirstly, let’s compare the two models:\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = depression ~ group, data = dat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.7678 -0.7283 -0.1033  0.6176  3.7671 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   0.5795     0.1335   4.341 2.62e-05 ***\n#&gt; groupg2       0.3170     0.1888   1.679   0.0952 .  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.156 on 148 degrees of freedom\n#&gt; Multiple R-squared:  0.0187, Adjusted R-squared:  0.01207 \n#&gt; F-statistic:  2.82 on 1 and 148 DF,  p-value: 0.09521"
  },
  {
    "objectID": "slides/bayesian-glm.html#brms-results",
    "href": "slides/bayesian-glm.html#brms-results",
    "title": "Generalized Linear Models with brms",
    "section": "brms results",
    "text": "brms results\nFirsly we can have a look at the posterior distributions of the parameters:\n\nplot(fit_group)"
  },
  {
    "objectID": "slides/bayesian-glm.html#model-checking-using-simulations",
    "href": "slides/bayesian-glm.html#model-checking-using-simulations",
    "title": "Generalized Linear Models with brms",
    "section": "Model checking using simulations",
    "text": "Model checking using simulations\nWe can check the model fit using simulations. In Bayesian terms this is called Posterior Predictive Checks. For standard models we use only the likelihood."
  },
  {
    "objectID": "slides/bayesian-glm.html#model-checking-using-simulations-1",
    "href": "slides/bayesian-glm.html#model-checking-using-simulations-1",
    "title": "Generalized Linear Models with brms",
    "section": "Model checking using simulations",
    "text": "Model checking using simulations\nWith the Bayesian models we can just use the brms::pp_check() function that compute the posterior predictive checks:\n\npp_check(fit_group)"
  },
  {
    "objectID": "slides/bayesian-glm.html#setting-priors",
    "href": "slides/bayesian-glm.html#setting-priors",
    "title": "Generalized Linear Models with brms",
    "section": "Setting priors",
    "text": "Setting priors\nBy default brms use some priors. You can see the actual used priors using:\n\n\n#&gt;                   prior     class    coef group resp dpar nlpar lb ub\n#&gt;                  (flat)         b                                    \n#&gt;                  (flat)         b groupg2                            \n#&gt;  student_t(3, 0.9, 2.5) Intercept                                    \n#&gt;    student_t(3, 0, 2.5)     sigma                                0   \n#&gt;        source\n#&gt;       default\n#&gt;  (vectorized)\n#&gt;       default\n#&gt;       default\n\n\nYou can also see the priors before fitting the model:\n\n\n#&gt;                   prior     class    coef group resp dpar nlpar lb ub\n#&gt;                  (flat)         b                                    \n#&gt;                  (flat)         b groupg2                            \n#&gt;  student_t(3, 0.6, 2.5) Intercept                                    \n#&gt;    student_t(3, 0, 2.5)     sigma                                0   \n#&gt;        source\n#&gt;       default\n#&gt;  (vectorized)\n#&gt;       default\n#&gt;       default"
  },
  {
    "objectID": "slides/bayesian-glm.html#centering-re-scaling-and-contrasts-coding-1",
    "href": "slides/bayesian-glm.html#centering-re-scaling-and-contrasts-coding-1",
    "title": "Generalized Linear Models with brms",
    "section": "Centering, re-scaling and contrasts coding",
    "text": "Centering, re-scaling and contrasts coding\nWhen fitting a model is important to transform the predictors according to the hypothesis that we have and the intepretation of parameters.\nCentering (for numerical variables) and contrasts coding (for categorical variables) are the two main strategies affecting the intepretation of model parameters.\nThe crucial point is that also the prior distribution need to be adapted when using different parametrizations of the same model."
  },
  {
    "objectID": "slides/bayesian-glm.html#rescaling",
    "href": "slides/bayesian-glm.html#rescaling",
    "title": "Generalized Linear Models with brms",
    "section": "Rescaling",
    "text": "Rescaling\nFor example, let’s assume to have the relationship between self-esteem (from 0 to 20) and the graduation mark from 66 to 111 (110 cum laude):"
  },
  {
    "objectID": "slides/bayesian-glm.html#rescaling-1",
    "href": "slides/bayesian-glm.html#rescaling-1",
    "title": "Generalized Linear Models with brms",
    "section": "Rescaling",
    "text": "Rescaling\nLet’s fit a simple regression:\n\n\n#&gt;  Family: gaussian \n#&gt;   Links: mu = identity; sigma = identity \n#&gt; Formula: se ~ mark \n#&gt;    Data: dat_mark (Number of observations: 30) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Regression Coefficients:\n#&gt;           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept    -7.96      2.10   -12.20    -3.85 1.00     3159     2822\n#&gt; mark          0.13      0.02     0.09     0.18 1.00     3186     2813\n#&gt; \n#&gt; Further Distributional Parameters:\n#&gt;       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; sigma     1.63      0.22     1.27     2.11 1.00     2741     2599\n#&gt; \n#&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential\n#&gt; scale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides/bayesian-glm.html#rescaling-problems",
    "href": "slides/bayesian-glm.html#rescaling-problems",
    "title": "Generalized Linear Models with brms",
    "section": "Rescaling, problems?",
    "text": "Rescaling, problems?\nWe are using the default priors that are basically non-informative. What about setting appropriate or more informative priors?\n\n\n#&gt;                   prior     class coef group resp dpar nlpar lb ub       source\n#&gt;                  (flat)         b                                       default\n#&gt;                  (flat)         b mark                             (vectorized)\n#&gt;  student_t(3, 3.9, 2.5) Intercept                                       default\n#&gt;    student_t(3, 0, 2.5)     sigma                             0         default"
  },
  {
    "objectID": "slides/bayesian-glm.html#rescaling-problems-1",
    "href": "slides/bayesian-glm.html#rescaling-problems-1",
    "title": "Generalized Linear Models with brms",
    "section": "Rescaling, problems?",
    "text": "Rescaling, problems?\nThere are a couple of problems:\n\nIntercept is the expected self-esteem score for people with 0 graduation mark (is that plausible?)\nmark is the expected increase in self-esteem for a unit increase in the graduation mark. (is that intepretable?)\n\nAssuming that we want to put priors, how do you choose the distribution and the parameters?"
  },
  {
    "objectID": "slides/bayesian-glm.html#rescaling-problems-2",
    "href": "slides/bayesian-glm.html#rescaling-problems-2",
    "title": "Generalized Linear Models with brms",
    "section": "Rescaling, problems?",
    "text": "Rescaling, problems?\nThe first problem is that the Intercept is meaningless. Thus we can, for example, mean-center the mark variable. The slope is the same, we are only shifting the x. The intercept is different."
  },
  {
    "objectID": "slides/bayesian-glm.html#intercept-prior",
    "href": "slides/bayesian-glm.html#intercept-prior",
    "title": "Generalized Linear Models with brms",
    "section": "Intercept prior",
    "text": "Intercept prior\nNow the intercept is the expected self esteem value when mark is on average. Given that the values ranges from 0 to 20, we could put less probability of extreme values (for average marks, around 88) we could imagine also average value for self-esteem (around 10)."
  },
  {
    "objectID": "slides/bayesian-glm.html#intercept-prior-centering",
    "href": "slides/bayesian-glm.html#intercept-prior-centering",
    "title": "Generalized Linear Models with brms",
    "section": "Intercept prior, centering",
    "text": "Intercept prior, centering\n\npriors &lt;- c(\n  prior(normal(10, 8), class = \"Intercept\")\n)\n\npriors\n\n#&gt; Intercept ~ normal(10, 8)"
  },
  {
    "objectID": "slides/bayesian-glm.html#slope-prior-rescaling",
    "href": "slides/bayesian-glm.html#slope-prior-rescaling",
    "title": "Generalized Linear Models with brms",
    "section": "Slope prior, rescaling",
    "text": "Slope prior, rescaling\nThen for the slope, probably there is too much granularity in the mark variable. 1 point increase is very tiny. To improve the model interpretation we can rescale the variable giving more weight to the unit increase."
  },
  {
    "objectID": "slides/bayesian-glm.html#slope-prior-rescaling-1",
    "href": "slides/bayesian-glm.html#slope-prior-rescaling-1",
    "title": "Generalized Linear Models with brms",
    "section": "Slope prior, rescaling",
    "text": "Slope prior, rescaling\nNow we have a more practical idea of size of the slope. We can use a very vague but not flat prior (the default) considering that 0 means no effect. Remember that now the slope is the increase in self-esteem for incrase of 10 points in the mark."
  },
  {
    "objectID": "slides/bayesian-glm.html#slope-prior-rescaling-2",
    "href": "slides/bayesian-glm.html#slope-prior-rescaling-2",
    "title": "Generalized Linear Models with brms",
    "section": "Slope prior, rescaling",
    "text": "Slope prior, rescaling\nThe previus prior is very uninformative but is simply excluding impossible values. A slope of 10 means that increasing by 10 points would produce an increase that ranges the entire available scale."
  },
  {
    "objectID": "slides/bayesian-glm.html#refitting-the-model",
    "href": "slides/bayesian-glm.html#refitting-the-model",
    "title": "Generalized Linear Models with brms",
    "section": "Refitting the model1",
    "text": "Refitting the model1\nWe can now refit the model using our priors and rescaling/centering\n\n\n#&gt;  Family: gaussian \n#&gt;   Links: mu = identity; sigma = identity \n#&gt; Formula: se ~ mark10c \n#&gt;    Data: dat_mark (Number of observations: 30) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Regression Coefficients:\n#&gt;           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept     3.58      0.32     2.95     4.23 1.00     3172     2437\n#&gt; mark10c       0.60      0.21     0.18     0.99 1.00     3775     2708\n#&gt; \n#&gt; Further Distributional Parameters:\n#&gt;       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; sigma     1.75      0.24     1.35     2.31 1.00     3378     2792\n#&gt; \n#&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential\n#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFor the residual standard deviation, brms is already doing a good job with default priors, mainly escluding negative values."
  },
  {
    "objectID": "slides/bayesian-glm.html#contrasts-coding",
    "href": "slides/bayesian-glm.html#contrasts-coding",
    "title": "Generalized Linear Models with brms",
    "section": "Contrasts coding",
    "text": "Contrasts coding\nContrasts coding is a vast and difficult topic. The basic idea is that when you have categorical predictors, with or without interactions, the way you set the contrasts will impact the intepretation of model parameters (and the priors).\nBy default in R, categorical variables are coded using the so-called dummy coding or treatment coding.\n\nx &lt;- factor(rep(c(\"a\", \"b\", \"c\"), each = 5))\nx\n\n#&gt;  [1] a a a a a b b b b b c c c c c\n#&gt; Levels: a b c"
  },
  {
    "objectID": "slides/bayesian-glm.html#contrasts-coding-1",
    "href": "slides/bayesian-glm.html#contrasts-coding-1",
    "title": "Generalized Linear Models with brms",
    "section": "Contrasts coding",
    "text": "Contrasts coding\n\ncontrasts(x)\n\n#&gt;   b c\n#&gt; a 0 0\n#&gt; b 1 0\n#&gt; c 0 1\n\nmodel.matrix(~x)\n\n#&gt;    (Intercept) xb xc\n#&gt; 1            1  0  0\n#&gt; 2            1  0  0\n#&gt; 3            1  0  0\n#&gt; 4            1  0  0\n#&gt; 5            1  0  0\n#&gt; 6            1  1  0\n#&gt; 7            1  1  0\n#&gt; 8            1  1  0\n#&gt; 9            1  1  0\n#&gt; 10           1  1  0\n#&gt; 11           1  0  1\n#&gt; 12           1  0  1\n#&gt; 13           1  0  1\n#&gt; 14           1  0  1\n#&gt; 15           1  0  1\n#&gt; attr(,\"assign\")\n#&gt; [1] 0 1 1\n#&gt; attr(,\"contrasts\")\n#&gt; attr(,\"contrasts\")$x\n#&gt; [1] \"contr.treatment\""
  },
  {
    "objectID": "slides/bayesian-glm.html#contrasts-coding-2",
    "href": "slides/bayesian-glm.html#contrasts-coding-2",
    "title": "Generalized Linear Models with brms",
    "section": "Contrasts coding",
    "text": "Contrasts coding\nWith a factor with \\(p\\) levels, we need \\(p - 1\\) variables representing contrasts. dummy-coding means that there will be a reference level (usually the first level) and \\(p - 1\\) contrasts comparing the other levels with the first level (baseline).\nAn example with the iris dataset:\n\nlevels(iris$Species)\n#&gt; [1] \"setosa\"     \"versicolor\" \"virginica\"\nfit_dummy &lt;- lm(Sepal.Length ~ Species, data = iris)\nsummary(fit_dummy)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Sepal.Length ~ Species, data = iris)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.6880 -0.3285 -0.0060  0.3120  1.3120 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)         5.0060     0.0728  68.762  &lt; 2e-16 ***\n#&gt; Speciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\n#&gt; Speciesvirginica    1.5820     0.1030  15.366  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.5148 on 147 degrees of freedom\n#&gt; Multiple R-squared:  0.6187, Adjusted R-squared:  0.6135 \n#&gt; F-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "slides/bayesian-glm.html#contrasts-coding-3",
    "href": "slides/bayesian-glm.html#contrasts-coding-3",
    "title": "Generalized Linear Models with brms",
    "section": "Contrasts coding",
    "text": "Contrasts coding\n\nvv &lt;- split(iris$Sepal.Length, iris$Species)\nmean(vv$setosa)\n\n#&gt; [1] 5.006\n\nmean(vv$versicolor) - mean(vv$setosa)\n\n#&gt; [1] 0.93\n\nmean(vv$virginica) - mean(vv$setosa)\n\n#&gt; [1] 1.582"
  },
  {
    "objectID": "slides/bayesian-glm.html#contrasts-coding-4",
    "href": "slides/bayesian-glm.html#contrasts-coding-4",
    "title": "Generalized Linear Models with brms",
    "section": "Contrasts coding",
    "text": "Contrasts coding\nAnother coding scheme could be the so called Successive Differences Contrast Coding. The idea is to compare level 2 with level 1, level 3 with level 2 and so on.\n\niris$Species_sdif &lt;- iris$Species\ncontrasts(iris$Species_sdif) &lt;- MASS::contr.sdif(3)\ncontrasts(iris$Species_sdif)\n\n#&gt;                   2-1        3-2\n#&gt; setosa     -0.6666667 -0.3333333\n#&gt; versicolor  0.3333333 -0.3333333\n#&gt; virginica   0.3333333  0.6666667"
  },
  {
    "objectID": "slides/bayesian-glm.html#contrasts-coding-5",
    "href": "slides/bayesian-glm.html#contrasts-coding-5",
    "title": "Generalized Linear Models with brms",
    "section": "Contrasts coding",
    "text": "Contrasts coding\n\nfit_sdif &lt;- lm(Sepal.Length ~ Species_sdif, data = iris)\nsummary(fit_sdif)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Sepal.Length ~ Species_sdif, data = iris)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.6880 -0.3285 -0.0060  0.3120  1.3120 \n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)      5.84333    0.04203 139.020  &lt; 2e-16 ***\n#&gt; Species_sdif2-1  0.93000    0.10296   9.033 8.77e-16 ***\n#&gt; Species_sdif3-2  0.65200    0.10296   6.333 2.77e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.5148 on 147 degrees of freedom\n#&gt; Multiple R-squared:  0.6187, Adjusted R-squared:  0.6135 \n#&gt; F-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "slides/bayesian-glm.html#more-on-contrasts-coding",
    "href": "slides/bayesian-glm.html#more-on-contrasts-coding",
    "title": "Generalized Linear Models with brms",
    "section": "More on contrasts coding",
    "text": "More on contrasts coding\nThere are few very useful papers about contrasts coding:\n\nSchad et al. (2020): comprehensive and (difficult) paper about contrasts coding\nGranziol et al. (2025): amazing work by our colleagues in Padova"
  },
  {
    "objectID": "slides/bayesian-glm.html#hypothesis-testing",
    "href": "slides/bayesian-glm.html#hypothesis-testing",
    "title": "Generalized Linear Models with brms",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nThe easiest way to test an hypothesis similarly to the frequentist framework is by checking if the null value of a certain test is contaned or not in the Credible Interval or the Highest Posterior Density Interval.\nIn the frequentist framework, the p value lower than \\(\\alpha\\) corresponds to a confidence interval to \\(1 - \\alpha\\) level that does not contains the null value (e.g., 0)."
  },
  {
    "objectID": "slides/bayesian-glm.html#brmshypothesis",
    "href": "slides/bayesian-glm.html#brmshypothesis",
    "title": "Generalized Linear Models with brms",
    "section": "brms::hypothesis()",
    "text": "brms::hypothesis()\nThe brms::hypothesis() function is a very nice way to test hypotheses into a bayesian framework."
  },
  {
    "objectID": "slides/bayesian-glm.html#bayesian-r2-gelman2019-hp",
    "href": "slides/bayesian-glm.html#bayesian-r2-gelman2019-hp",
    "title": "Generalized Linear Models with brms",
    "section": "Bayesian \\(R^2\\) (Gelman et al. 2019)",
    "text": "Bayesian \\(R^2\\) (Gelman et al. 2019)\nGelman et al. (2019) explained a generalization of the common \\(R^2\\) to be applied for Bayesian Generalized Linear Models.\n\\[\n\\text{Bayesian } R^2_s =\n\\frac{\n\\mathrm{Var}_{n=1}^{N}\\left( y_n^{\\text{pred}, s} \\right)\n}{\n\\mathrm{Var}_{n=1}^{N}\\left( y_n^{\\text{pred}, s} \\right) + \\mathrm{Var}_{\\text{res}}^s\n}\n\\]\nThere are few important points:\n\nThis works for any GLM (unlike the usual \\(R^2\\))\nDifferent models on the same dataset cannot be compared  https://avehtari.github.io/bayes_R2/bayes_R2.html"
  },
  {
    "objectID": "slides/bayesian-glm.html#extracting-posteriors",
    "href": "slides/bayesian-glm.html#extracting-posteriors",
    "title": "Generalized Linear Models with brms",
    "section": "Extracting posteriors",
    "text": "Extracting posteriors\nhttps://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/ https://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/#complete-cheat-sheet"
  },
  {
    "objectID": "slides/bayesian-glm.html#bayes-factor",
    "href": "slides/bayesian-glm.html#bayes-factor",
    "title": "Generalized Linear Models with brms",
    "section": "Bayes Factor",
    "text": "Bayes Factor\nAs an example we can start with the classical coin-flip experiment. We need to guess if a coin is fair or not. Firstly let’s formalize our prior beliefs in probabilistic terms:"
  },
  {
    "objectID": "slides/bayesian-glm.html#bayes-factor-1",
    "href": "slides/bayesian-glm.html#bayes-factor-1",
    "title": "Generalized Linear Models with brms",
    "section": "Bayes Factor",
    "text": "Bayes Factor\nNow we collect data and we observe \\(x = 40\\) tails out of \\(k = 50\\) trials thus \\(\\hat{\\pi} = 0.8\\) and compute the likelihood:"
  },
  {
    "objectID": "slides/bayesian-glm.html#bayes-factor-2",
    "href": "slides/bayesian-glm.html#bayes-factor-2",
    "title": "Generalized Linear Models with brms",
    "section": "Bayes Factor",
    "text": "Bayes Factor\nFinally we combine, using the Bayes rule, prior and likelihood to obtain the posterior distribution:"
  },
  {
    "objectID": "slides/bayesian-glm.html#bayes-factor-3",
    "href": "slides/bayesian-glm.html#bayes-factor-3",
    "title": "Generalized Linear Models with brms",
    "section": "Bayes Factor",
    "text": "Bayes Factor\nThe Bayes Factor (BF) — also called the likelihood ratio, for obvious reasons — is a measure of the relative support that the evidence provides for two competing hypotheses, \\(H_0\\) and \\(H_1\\) (~ \\(\\pi\\) in our previous example). It plays a key role in the following odds form of Bayes’s theorem.\n\\[\n\\frac{p(H_0|D)}{p(H_1|D)} = \\frac{p(D|H_0)}{p(D|H_1)} \\times \\frac{p(H_0)}{p(H_1)}\n\\]\nThe ratio of the priors \\(\\frac{p(H_0)}{p(H_1)}\\) is called the prior odds of the hypotheses; and, the ratio of the poosteriors \\(\\frac{p(H_0| D)}{p(H_1 | D)}\\) is called the posterior odds of the hypotheses. Thus, the above (odds form) of Bayes’s Theorem can be paraphrased as follows\n\\[\n\\text{posterior odds} = \\text{Bayes Factor} \\times \\text{prior odds}\n\\]"
  },
  {
    "objectID": "slides/bayesian-glm.html#calculating-the-bayes-factor-using-the-sdr",
    "href": "slides/bayesian-glm.html#calculating-the-bayes-factor-using-the-sdr",
    "title": "Generalized Linear Models with brms",
    "section": "Calculating the Bayes Factor using the SDR",
    "text": "Calculating the Bayes Factor using the SDR\nCalculating the BF can be challenging in some situations. The Savage-Dickey density ratio (SDR) is a convenient shortcut to calculate the Bayes Factor (Wagenmakers et al. 2010). The idea is that the ratio of the prior and posterior density distribution for hypothesis \\(H_1\\) is an estimate of the Bayes factor calculated in the standard way.\n\\[\nBF_{01} = \\frac{p(D|H_0)}{p(D|H_1)} \\approx \\frac{p(\\pi = x|D, H_1)}{p(\\pi = x | H_1)}\n\\]\nWhere \\(\\pi\\) is the parameter of interest and \\(x\\) is the null value under \\(H_0\\) (e.g., 0). and \\(D\\) are the data."
  },
  {
    "objectID": "slides/bayesian-glm.html#calculating-the-bayes-factor-using-the-sdr-1",
    "href": "slides/bayesian-glm.html#calculating-the-bayes-factor-using-the-sdr-1",
    "title": "Generalized Linear Models with brms",
    "section": "Calculating the Bayes Factor using the SDR",
    "text": "Calculating the Bayes Factor using the SDR\nFollowing the previous example \\(H_0: \\pi = 0.5\\). Under \\(H_1\\) we use a vague prior by setting \\(\\pi \\sim Beta(1, 1)\\).\nSay we flipped the coin 20 times and we found that \\(\\hat \\pi = 0.75\\)."
  },
  {
    "objectID": "slides/bayesian-glm.html#calculating-the-bayes-factor-using-the-sdr-2",
    "href": "slides/bayesian-glm.html#calculating-the-bayes-factor-using-the-sdr-2",
    "title": "Generalized Linear Models with brms",
    "section": "Calculating the Bayes Factor using the SDR",
    "text": "Calculating the Bayes Factor using the SDR\nThe ratio between the two black dots is the Bayes Factor."
  },
  {
    "objectID": "slides/bayesian-glm.html#bayes-factor-in-brms",
    "href": "slides/bayesian-glm.html#bayes-factor-in-brms",
    "title": "Generalized Linear Models with brms",
    "section": "Bayes Factor in brms",
    "text": "Bayes Factor in brms\nhttps://vuorre.com/posts/2017-03-21-bayes-factors-with-brms/ https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html#:~:text=For%20the%20computation%20of%20Bayes,flat%20priors%20the%20null%20is"
  },
  {
    "objectID": "slides/bayesian-glm.html#be-careful-with-the-bayes-factor",
    "href": "slides/bayesian-glm.html#be-careful-with-the-bayes-factor",
    "title": "Generalized Linear Models with brms",
    "section": "Be careful with the Bayes Factor",
    "text": "Be careful with the Bayes Factor\nThe Bayes Factor computed with hypothesis() or in general the SDR method is highly sensitive to the priors. See also the bayestestR documentation. In general:\n\nThe Bayes Factor requires informative or at least non-flat priors. Remember that in brms the default prior is flat\nAs the prior scale (e.g., standard deviation) increase, the Bayes Factor tends to suggest evidence for the null hypothesis, even when the null hypothesis is false\n\nThis is called Lindley’s paradox. (see Wagenmakers and Ly 2023)."
  },
  {
    "objectID": "slides/bayesian-glm.html#lindleys-paradox-a-simulation",
    "href": "slides/bayesian-glm.html#lindleys-paradox-a-simulation",
    "title": "Generalized Linear Models with brms",
    "section": "Lindley’s paradox, a simulation",
    "text": "Lindley’s paradox, a simulation\nWe can just simulate a t-test (or equivalently a parameter of a model) and run the Bayesian model with different prior distribution scale.\nFor a t-test we are focused on the prior for the (standardized) difference between the means. We can set a prior centered on 0 with different scale.\nHere I’m using the BayesFactor package just because is faster for simple model if we are interested in the Bayes Factor pointnull (the same as brms::hypothesis(x, \"b = 0\"))."
  },
  {
    "objectID": "slides/bayesian-glm.html#lindleys-paradox-a-simulation-1",
    "href": "slides/bayesian-glm.html#lindleys-paradox-a-simulation-1",
    "title": "Generalized Linear Models with brms",
    "section": "Lindley’s paradox, a simulation",
    "text": "Lindley’s paradox, a simulation"
  },
  {
    "objectID": "slides/bayesian-glm.html#prior-sensitivity-check",
    "href": "slides/bayesian-glm.html#prior-sensitivity-check",
    "title": "Generalized Linear Models with brms",
    "section": "Prior Sensitivity Check",
    "text": "Prior Sensitivity Check\nGiven the sensitivity of Bayesian models to the prior, especially when informative is always a good idea to check the actual impact of priors. Not only for Bayes Factors but also for the posterior distributions.\n\nRun your model with the prior that are more plausible for you\nRun the same model with priors that are more and less informative\nCheck the impact of the prior on your posterior distributions and conclusions"
  },
  {
    "objectID": "slides/bayesian-glm.html#prior-predictive-check",
    "href": "slides/bayesian-glm.html#prior-predictive-check",
    "title": "Generalized Linear Models with brms",
    "section": "Prior Predictive Check",
    "text": "Prior Predictive Check\nThe prior predictive check is an important process when setting the priors in a regression model. The basic idea is that the posterior distribution is the combination between priors and likelihood (i.e., the data). If we remove likelihood from the equation, we obtain a “posterior” only using information in the prior.\nThe advantage is that we can simulate data to check if our priors are reasonable according to our model, phenomenon, etc.\nIn brms there is an argument called sample_prior that can be set to \"only\" to fit a model ignoring the data:\n\nbrm(\n  y ~ x,\n  data = dat, # not used\n  sample_prior = \"only\"\n)"
  },
  {
    "objectID": "slides/bayesian-glm.html#prior-predictive-check-1",
    "href": "slides/bayesian-glm.html#prior-predictive-check-1",
    "title": "Generalized Linear Models with brms",
    "section": "Prior predictive check",
    "text": "Prior predictive check\nAssuming to have a simple model with a continous and categorical predictor:\n\n\n#&gt;            y          x g\n#&gt; 1  0.7812706  0.6207567 0\n#&gt; 2 -1.1101886  0.0356414 0\n#&gt; 3  0.3243795  0.7731545 0\n#&gt; 4  1.8243161  1.2724891 0\n#&gt; 5  0.6446891  0.3709754 0\n#&gt; 6  1.0857640 -0.1628543 0"
  },
  {
    "objectID": "slides/bayesian-glm.html#prior-predictive-check-2",
    "href": "slides/bayesian-glm.html#prior-predictive-check-2",
    "title": "Generalized Linear Models with brms",
    "section": "Prior predictive check",
    "text": "Prior predictive check\nWe know that x and y are standandardized thus we can think about the effect g in terms of Cohen’s \\(d\\) and the effect of x in terms of units of standard deviations. Let’s set some priors for \\(\\beta_1\\) and \\(\\beta_2\\).\n\nget_prior(y ~ x + g, data = dat)\n\n#&gt;                 prior     class coef group resp dpar nlpar lb ub       source\n#&gt;                (flat)         b                                       default\n#&gt;                (flat)         b    g                             (vectorized)\n#&gt;                (flat)         b    x                             (vectorized)\n#&gt;  student_t(3, 0, 2.5) Intercept                                       default\n#&gt;  student_t(3, 0, 2.5)     sigma                             0         default"
  },
  {
    "objectID": "slides/bayesian-glm.html#prior-predictive-check-3",
    "href": "slides/bayesian-glm.html#prior-predictive-check-3",
    "title": "Generalized Linear Models with brms",
    "section": "Prior predictive check",
    "text": "Prior predictive check\n\npriors &lt;- c(\n  prior(normal(0, 1), class = \"b\", coef = \"g\"),\n  prior(normal(0, 2), class = \"b\", coef = \"x\")\n)\n\nfit_prior &lt;- brm(y ~ x + g, data = dat, sample_prior = \"only\", prior = priors, file = here(\"slides\", \"objects\", \"fit_prior.rds\"))\nsummary(fit_prior)\n\n#&gt;  Family: gaussian \n#&gt;   Links: mu = identity; sigma = identity \n#&gt; Formula: y ~ x + g \n#&gt;    Data: dat (Number of observations: 50) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Regression Coefficients:\n#&gt;           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept     0.30      5.00    -8.67     8.82 1.00     2585     1590\n#&gt; x             0.03      2.04    -4.00     3.96 1.00     2844     2449\n#&gt; g            -0.02      1.00    -2.05     1.91 1.00     3089     2397\n#&gt; \n#&gt; Further Distributional Parameters:\n#&gt;       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; sigma     2.79      3.83     0.08    10.10 1.00     2647     1440\n#&gt; \n#&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential\n#&gt; scale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides/bayesian-glm.html#prior-predictive-check-4",
    "href": "slides/bayesian-glm.html#prior-predictive-check-4",
    "title": "Generalized Linear Models with brms",
    "section": "Prior predictive check",
    "text": "Prior predictive check"
  },
  {
    "objectID": "slides/bayesian-glm.html#extracting-posterior-distributions",
    "href": "slides/bayesian-glm.html#extracting-posterior-distributions",
    "title": "Generalized Linear Models with brms",
    "section": "Extracting Posterior Distributions",
    "text": "Extracting Posterior Distributions\nWhen running a model with lm and glm, all the required information is included into the summary of the model. For each parameter we have the point estimate, standard error, confidence interval, etc.\n\nfit_lm_example &lt;- lm(Sepal.Length ~ Petal.Width + Species, data = iris)\nsummary(fit_lm_example)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Sepal.Length ~ Petal.Width + Species, data = iris)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.3891 -0.3043 -0.0472  0.2528  1.3358 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)        4.78044    0.08308  57.543  &lt; 2e-16 ***\n#&gt; Petal.Width        0.91690    0.19386   4.730 5.25e-06 ***\n#&gt; Speciesversicolor -0.06025    0.23041  -0.262    0.794    \n#&gt; Speciesvirginica  -0.05009    0.35823  -0.140    0.889    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.481 on 146 degrees of freedom\n#&gt; Multiple R-squared:  0.6694, Adjusted R-squared:  0.6626 \n#&gt; F-statistic: 98.53 on 3 and 146 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "slides/bayesian-glm.html#extracting-posterior-distributions-1",
    "href": "slides/bayesian-glm.html#extracting-posterior-distributions-1",
    "title": "Generalized Linear Models with brms",
    "section": "Extracting Posterior Distributions",
    "text": "Extracting Posterior Distributions\nWith Bayesian models we have posterior distributions that can be summarised in different ways. There are several methods and packages to work with posteriors:"
  },
  {
    "objectID": "slides/bayesian-glm.html#extracting-posterior-distributions-2",
    "href": "slides/bayesian-glm.html#extracting-posterior-distributions-2",
    "title": "Generalized Linear Models with brms",
    "section": "Extracting Posterior Distributions",
    "text": "Extracting Posterior Distributions\nWith the as_draws_df we can extract all the samples from the posterior distribution:\n\n\n#&gt; # A draws_df: 6 iterations, 1 chains, and 6 variables\n#&gt;   b_Intercept b_mark sigma Intercept lprior lp__\n#&gt; 1        -9.6   0.15   1.5       4.3   -3.4  -59\n#&gt; 2       -10.2   0.16   1.4       4.1   -3.3  -59\n#&gt; 3        -9.0   0.15   1.5       4.6   -3.4  -62\n#&gt; 4        -9.0   0.15   1.4       4.3   -3.4  -60\n#&gt; 5        -8.7   0.14   1.5       3.8   -3.4  -59\n#&gt; 6        -8.0   0.13   1.4       3.7   -3.3  -60\n#&gt; # ... hidden reserved variables {'.chain', '.iteration', '.draw'}"
  },
  {
    "objectID": "slides/bayesian-glm.html#extracting-posterior-distributions-3",
    "href": "slides/bayesian-glm.html#extracting-posterior-distributions-3",
    "title": "Generalized Linear Models with brms",
    "section": "Extracting Posterior Distributions",
    "text": "Extracting Posterior Distributions\n\nas_draws_df(fit_mark) |&gt; \n  select(starts_with(\"b\"), sigma) |&gt; \n  posterior::summarise_draws()\n\n#&gt; # A tibble: 3 × 10\n#&gt;   variable     mean median     sd    mad       q5    q95  rhat ess_bulk ess_tail\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 b_Interce… -7.96  -7.98  2.10   2.05   -11.5    -4.49   1.00    3049.    2798.\n#&gt; 2 b_mark      0.133  0.133 0.0230 0.0227   0.0954  0.172  1.00    3064.    2776.\n#&gt; 3 sigma       1.63   1.60  0.217  0.211    1.32    2.03   1.00    2706.    2588."
  },
  {
    "objectID": "slides/bayesian-glm.html#hpdi-with-bayestestr",
    "href": "slides/bayesian-glm.html#hpdi-with-bayestestr",
    "title": "Generalized Linear Models with brms",
    "section": "HPDI with bayestestR",
    "text": "HPDI with bayestestR\nThe bayestestR contains a lot of functions to do inference and post-processing on brms (and also other) models:\n\nbayestestR::hdi(fit_mark)\n\n#&gt; Highest Density Interval\n#&gt; \n#&gt; Parameter   |         95% HDI\n#&gt; -----------------------------\n#&gt; (Intercept) | [-12.24, -3.92]\n#&gt; mark        | [  0.09,  0.18]\n\nplot(bayestestR::hdi(fit_mark))"
  },
  {
    "objectID": "slides/bayesian-glm.html#the-broom-package",
    "href": "slides/bayesian-glm.html#the-broom-package",
    "title": "Generalized Linear Models with brms",
    "section": "The broom package",
    "text": "The broom package\nThe broom.mixed package automatically provide the summary() into a data.frame format:\n\nlibrary(broom.mixed)\nbroom.mixed::tidy(fit_mark)\n\n#&gt; # A tibble: 3 × 8\n#&gt;   effect   component group    term         estimate std.error conf.low conf.high\n#&gt;   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 fixed    cond      &lt;NA&gt;     (Intercept)    -7.96     2.10   -12.2       -3.85 \n#&gt; 2 fixed    cond      &lt;NA&gt;     mark            0.133    0.0230   0.0882     0.179\n#&gt; 3 ran_pars cond      Residual sd__Observa…    1.63     0.217    1.27       2.11"
  },
  {
    "objectID": "slides/bayesian-glm.html#more-on-bayestestr",
    "href": "slides/bayesian-glm.html#more-on-bayestestr",
    "title": "Generalized Linear Models with brms",
    "section": "More on bayestestR",
    "text": "More on bayestestR\nThis is a really amazing package, have a look at the Features section because contains functions and explanations about different ways of summarizing the posterior distribution."
  },
  {
    "objectID": "slides/bayesian-glm.html#plotting-effects",
    "href": "slides/bayesian-glm.html#plotting-effects",
    "title": "Generalized Linear Models with brms",
    "section": "Plotting effects",
    "text": "Plotting effects\nThe brms package contains some functions to plot the effects, such as the conditional_effects() function.\n\nconditional_effects(fit_mark)"
  },
  {
    "objectID": "slides/bayesian-glm.html#factorial-design-example-1",
    "href": "slides/bayesian-glm.html#factorial-design-example-1",
    "title": "Generalized Linear Models with brms",
    "section": "Factorial design example",
    "text": "Factorial design example\nWe can make a very easy example with a 3x2 factorial design. Let’s load some data:\n\ndat_factorial &lt;- readRDS(here(\"data\", \"clean\", \"factorial.rds\"))\nhead(dat_factorial)\n\n#&gt;     Group Anxiety         DV\n#&gt; 1 Control     Low 0.82075674\n#&gt; 2 Control     Low 0.23564140\n#&gt; 3 Control     Low 0.97315448\n#&gt; 4 Control     Low 1.47248909\n#&gt; 5 Control     Low 0.57097543\n#&gt; 6 Control     Low 0.03714566"
  },
  {
    "objectID": "slides/bayesian-glm.html#factorial-design-example-2",
    "href": "slides/bayesian-glm.html#factorial-design-example-2",
    "title": "Generalized Linear Models with brms",
    "section": "Factorial design example",
    "text": "Factorial design example\nWe can fit our model with interaction:\n\nfit_factorial &lt;- brm(\n  DV ~ Group * Anxiety,\n  file = here(\"slides\", \"objects\", \"fit_factorial.rds\"),\n  data = dat_factorial\n)"
  },
  {
    "objectID": "slides/bayesian-glm.html#factorial-design-example-3",
    "href": "slides/bayesian-glm.html#factorial-design-example-3",
    "title": "Generalized Linear Models with brms",
    "section": "Factorial design example",
    "text": "Factorial design example\nWe can create the grid of values:\n\ngrid &lt;- expand.grid(\n  Group = unique(dat_factorial$Group),\n  Anxiety = unique(dat_factorial$Anxiety)\n)\n\nhead(grid)\n\n#&gt;          Group Anxiety\n#&gt; 1      Control     Low\n#&gt; 2 Experimental     Low\n#&gt; 3      Control  Medium\n#&gt; 4 Experimental  Medium\n#&gt; 5      Control    High\n#&gt; 6 Experimental    High"
  },
  {
    "objectID": "slides/bayesian-glm.html#factorial-design-example-4",
    "href": "slides/bayesian-glm.html#factorial-design-example-4",
    "title": "Generalized Linear Models with brms",
    "section": "Factorial design example",
    "text": "Factorial design example\nWe can calculate the cell-means for the Group, averaging over the level of the other factor i.e. the main effect. We can use the tidybayes package.\n\nlibrary(tidybayes)\ngrid |&gt; \n  add_epred_draws(object = fit_factorial) |&gt; \n  group_by(Group, .draw) |&gt; \n  summarise(y = mean(.epred)) |&gt; \n  group_by(Group) |&gt; \n  summarise(median_hdci(y)) |&gt; \n  head()\n\n#&gt; # A tibble: 2 × 7\n#&gt;   Group            y    ymin  ymax .width .point .interval\n#&gt;   &lt;fct&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n#&gt; 1 Control      0.185 -0.0671 0.446   0.95 median hdci     \n#&gt; 2 Experimental 0.128 -0.125  0.395   0.95 median hdci"
  },
  {
    "objectID": "slides/bayesian-glm.html#factorial-design-example-5",
    "href": "slides/bayesian-glm.html#factorial-design-example-5",
    "title": "Generalized Linear Models with brms",
    "section": "Factorial design example",
    "text": "Factorial design example\nSame but using emmeans:\n\nlibrary(emmeans)\nemmeans(fit_factorial, ~ Group)\n\n#&gt;  Group        emmean lower.HPD upper.HPD\n#&gt;  Control       0.185   -0.0638     0.448\n#&gt;  Experimental  0.128   -0.1315     0.386\n#&gt; \n#&gt; Results are averaged over the levels of: Anxiety \n#&gt; Point estimate displayed: median \n#&gt; HPD interval probability: 0.95"
  },
  {
    "objectID": "slides/bayesian-glm.html#factorial-design-example-6",
    "href": "slides/bayesian-glm.html#factorial-design-example-6",
    "title": "Generalized Linear Models with brms",
    "section": "Factorial design example",
    "text": "Factorial design example\nHow about effect sizes? We can calculate a Cohen’s \\(d\\) using the posterior distributions:\n\neffectsize::cohens_d(DV ~ Group, data = dat_factorial)\n\n#&gt; Cohen's d |        95% CI\n#&gt; -------------------------\n#&gt; 0.05      | [-0.30, 0.41]\n#&gt; \n#&gt; - Estimated using pooled SD."
  },
  {
    "objectID": "slides/bayesian-glm.html#factorial-design-example-7",
    "href": "slides/bayesian-glm.html#factorial-design-example-7",
    "title": "Generalized Linear Models with brms",
    "section": "Factorial design example",
    "text": "Factorial design example\n\ngrid |&gt; \n  add_epred_draws(object = fit_factorial, dpar = TRUE) |&gt; \n  group_by(Group, .draw) |&gt; \n  summarise(y = mean(.epred),\n            sigma = mean(sigma)) |&gt; \n  pivot_wider(names_from = Group, values_from = y) |&gt; \n  mutate(diff = Control - Experimental,\n         d = diff / sigma) |&gt; \n  summarise(median_hdi(d))\n\n#&gt; # A tibble: 1 × 6\n#&gt;        y   ymin  ymax .width .point .interval\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n#&gt; 1 0.0564 -0.311 0.409   0.95 median hdi"
  },
  {
    "objectID": "slides/bayesian-glm.html#factorial-design-example-t-test",
    "href": "slides/bayesian-glm.html#factorial-design-example-t-test",
    "title": "Generalized Linear Models with brms",
    "section": "Factorial design example, t-test",
    "text": "Factorial design example, t-test\nEven easier with a t-test, we can do it directly from the model parameters:\n\n\nCode\nx &lt;- rep(0:1, each = 50)\ny &lt;- 0.5 * x + rnorm(length(x))\ndat_t &lt;- data.frame(y, x)\nboxplot(y ~ x, data = dat_t)"
  },
  {
    "objectID": "slides/bayesian-glm.html#factorial-design-example-t-test-1",
    "href": "slides/bayesian-glm.html#factorial-design-example-t-test-1",
    "title": "Generalized Linear Models with brms",
    "section": "Factorial design example, t-test",
    "text": "Factorial design example, t-test\n\nfit_t &lt;- brm(\n  y ~ x,\n  file = here(\"slides\", \"objects\", \"fit_t.rds\"),\n  data = dat_t\n)\n\nsummary(fit_t)\n\n#&gt;  Family: gaussian \n#&gt;   Links: mu = identity; sigma = identity \n#&gt; Formula: y ~ x \n#&gt;    Data: dat_t (Number of observations: 100) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Regression Coefficients:\n#&gt;           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept     0.07      0.15    -0.23     0.37 1.00     4608     3224\n#&gt; x             0.40      0.22    -0.04     0.83 1.00     4228     2771\n#&gt; \n#&gt; Further Distributional Parameters:\n#&gt;       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; sigma     1.06      0.08     0.92     1.23 1.00     3959     2430\n#&gt; \n#&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential\n#&gt; scale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides/bayesian-glm.html#factorial-design-example-t-test-2",
    "href": "slides/bayesian-glm.html#factorial-design-example-t-test-2",
    "title": "Generalized Linear Models with brms",
    "section": "Factorial design example, t-test",
    "text": "Factorial design example, t-test\n\npost &lt;- as_draws_df(fit_t)\nhead(post)\n\n#&gt; # A draws_df: 6 iterations, 1 chains, and 6 variables\n#&gt;   b_Intercept   b_x sigma Intercept lprior lp__\n#&gt; 1      -0.016 0.764   1.0      0.37   -3.3 -151\n#&gt; 2      -0.014 0.503   1.1      0.24   -3.3 -149\n#&gt; 3      -0.155 0.704   1.0      0.20   -3.2 -150\n#&gt; 4       0.326 0.097   1.1      0.37   -3.3 -151\n#&gt; 5       0.282 0.107   1.2      0.34   -3.3 -151\n#&gt; 6       0.313 0.151   1.2      0.39   -3.3 -151\n#&gt; # ... hidden reserved variables {'.chain', '.iteration', '.draw'}"
  },
  {
    "objectID": "slides/bayesian-glm.html#factorial-design-example-t-test-3",
    "href": "slides/bayesian-glm.html#factorial-design-example-t-test-3",
    "title": "Generalized Linear Models with brms",
    "section": "Factorial design example, t-test",
    "text": "Factorial design example, t-test\n\nhist(post$b_x / post$sigma)"
  },
  {
    "objectID": "slides/bayesian-glm.html#factorial-design-example-t-test-4",
    "href": "slides/bayesian-glm.html#factorial-design-example-t-test-4",
    "title": "Generalized Linear Models with brms",
    "section": "Factorial design example, t-test",
    "text": "Factorial design example, t-test\n\npost |&gt;\n  mutate(d = b_x / sigma) |&gt;\n  summarise(median_hdi(d))\n\n#&gt; # A tibble: 1 × 6\n#&gt;       y    ymin  ymax .width .point .interval\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n#&gt; 1 0.381 -0.0241 0.787   0.95 median hdi"
  },
  {
    "objectID": "slides/bayesian-glm.html#logistic-regression-example-1",
    "href": "slides/bayesian-glm.html#logistic-regression-example-1",
    "title": "Generalized Linear Models with brms",
    "section": "Logistic regression example",
    "text": "Logistic regression example\nFor an overview of the logistic regression we can refer to this material."
  },
  {
    "objectID": "slides/bayesian-glm.html#logistic-regression-using-brms",
    "href": "slides/bayesian-glm.html#logistic-regression-using-brms",
    "title": "Generalized Linear Models with brms",
    "section": "Logistic regression using brms",
    "text": "Logistic regression using brms\nWe can fit the same model of the previous slides using the teddy data.\n\nteddy &lt;- read.csv(here(\"data/clean/teddy_child.csv\"))\nhead(teddy)\n\n#&gt;   ID Children Fam_status        Education Occupation Age Smoking_status\n#&gt; 1  1        1    Married      High school  Full-time  42             No\n#&gt; 2  2        1    Married      High school  Full-time  42             No\n#&gt; 3  3        1    Married      High school  Full-time  44             No\n#&gt; 4  4        2    Married      High school  Full-time  48             No\n#&gt; 5  5        3    Married Degree or higher  Full-time  39            Yes\n#&gt; 6  6        2    Married Degree or higher  Full-time  38      Ex-smoker\n#&gt;   Alcool_status Depression_pp Parental_stress Alabama_involvment MSPSS_total\n#&gt; 1            No            No              75                 36    6.500000\n#&gt; 2            No            No              51                 41    5.500000\n#&gt; 3            No            No              76                 42    1.416667\n#&gt; 4            No            No              88                 39    1.083333\n#&gt; 5           Yes            No              67                 35    7.000000\n#&gt; 6            No            No              61                 36    3.333333\n#&gt;   LTE Partner_cohesion Family_cohesion\n#&gt; 1   5         4.333333        2.916667\n#&gt; 2   5         4.166667        3.000000\n#&gt; 3   4         3.000000        2.500000\n#&gt; 4   4         2.166667        2.666667\n#&gt; 5   2         4.166667        2.666667\n#&gt; 6   0         3.000000        2.583333"
  },
  {
    "objectID": "slides/bayesian-glm.html#logistic-regression-using-brms-1",
    "href": "slides/bayesian-glm.html#logistic-regression-using-brms-1",
    "title": "Generalized Linear Models with brms",
    "section": "Logistic regression using brms",
    "text": "Logistic regression using brms\nLet’s convert the response variable Depression_pp to a binary variable:\n\nteddy$Depression_pp01 &lt;- ifelse(teddy$Depression_pp == \"Yes\", 1, 0)\n\nfit_teddy &lt;- brm(\n  Depression_pp01 ~ Parental_stress,\n  data = teddy,\n  family = bernoulli(link = \"logit\"),\n  file = here(\"slides\", \"objects\", \"fit_teddy.rds\")\n)"
  },
  {
    "objectID": "slides/bayesian-glm.html#logistic-regression-using-brms-2",
    "href": "slides/bayesian-glm.html#logistic-regression-using-brms-2",
    "title": "Generalized Linear Models with brms",
    "section": "Logistic regression using brms",
    "text": "Logistic regression using brms\n\nsummary(fit_teddy)\n\n#&gt;  Family: bernoulli \n#&gt;   Links: mu = logit \n#&gt; Formula: Depression_pp01 ~ Parental_stress \n#&gt;    Data: teddy (Number of observations: 379) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Regression Coefficients:\n#&gt;                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept          -4.34      0.70    -5.71    -3.01 1.00     2570     2509\n#&gt; Parental_stress     0.04      0.01     0.02     0.06 1.00     2937     2486\n#&gt; \n#&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential\n#&gt; scale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides/bayesian-glm.html#logistic-regression-using-brms-3",
    "href": "slides/bayesian-glm.html#logistic-regression-using-brms-3",
    "title": "Generalized Linear Models with brms",
    "section": "Logistic regression using brms",
    "text": "Logistic regression using brms\n\nplot(fit_teddy)"
  },
  {
    "objectID": "slides/bayesian-glm.html#logistic-regression-using-brms-4",
    "href": "slides/bayesian-glm.html#logistic-regression-using-brms-4",
    "title": "Generalized Linear Models with brms",
    "section": "Logistic regression using brms",
    "text": "Logistic regression using brms\nThe posterior draws, by default, are always on the linear predictor scale. For the Gaussian model this is not relevant, but here everything is in logit.s\n\n\n#&gt; # A draws_df: 6 iterations, 1 chains, and 5 variables\n#&gt;   b_Intercept b_Parental_stress Intercept lprior lp__\n#&gt; 1        -4.9             0.045      -2.1   -2.3 -138\n#&gt; 2        -5.2             0.050      -2.1   -2.3 -139\n#&gt; 3        -3.6             0.024      -2.1   -2.3 -139\n#&gt; 4        -4.0             0.031      -2.1   -2.3 -138\n#&gt; 5        -3.9             0.029      -2.0   -2.3 -138\n#&gt; 6        -5.7             0.057      -2.0   -2.3 -141\n#&gt; # ... hidden reserved variables {'.chain', '.iteration', '.draw'}"
  },
  {
    "objectID": "slides/bayesian-glm.html#logistic-regression-using-brms-5",
    "href": "slides/bayesian-glm.html#logistic-regression-using-brms-5",
    "title": "Generalized Linear Models with brms",
    "section": "Logistic regression using brms",
    "text": "Logistic regression using brms\nThe intercept is the logit of the probability of success when all predictors are zero."
  },
  {
    "objectID": "slides/bayesian-glm.html#logistic-regression-using-brms-6",
    "href": "slides/bayesian-glm.html#logistic-regression-using-brms-6",
    "title": "Generalized Linear Models with brms",
    "section": "Logistic regression using brms",
    "text": "Logistic regression using brms\nThe Parental_stress coefficient is the increase in the log-odds of success for a unit increase in the Parental stress."
  },
  {
    "objectID": "slides/bayesian-glm.html#logistic-regression-using-brms-7",
    "href": "slides/bayesian-glm.html#logistic-regression-using-brms-7",
    "title": "Generalized Linear Models with brms",
    "section": "Logistic regression using brms",
    "text": "Logistic regression using brms\nWhen plotting the effects with conditional_effects() we are working on the probability space:\n\nconditional_effects(fit_teddy)"
  },
  {
    "objectID": "slides/bayesian-glm.html#logistic-regression-using-brms-8",
    "href": "slides/bayesian-glm.html#logistic-regression-using-brms-8",
    "title": "Generalized Linear Models with brms",
    "section": "Logistic regression using brms",
    "text": "Logistic regression using brms\n\nlibrary(tidybayes)\npp &lt;- data.frame(Parental_stress = c(40, 80, 120))\npredict(fit_teddy, newdata = pp)\n\n#&gt;      Estimate Est.Error Q2.5 Q97.5\n#&gt; [1,]  0.05875 0.2351856    0     1\n#&gt; [2,]  0.20125 0.4009846    0     1\n#&gt; [3,]  0.49250 0.5000063    0     1"
  },
  {
    "objectID": "slides/bayesian-glm.html#logistic-regression-using-brms-9",
    "href": "slides/bayesian-glm.html#logistic-regression-using-brms-9",
    "title": "Generalized Linear Models with brms",
    "section": "Logistic regression using brms",
    "text": "Logistic regression using brms"
  },
  {
    "objectID": "slides/bayesian-glm.html#bayesian-workflow",
    "href": "slides/bayesian-glm.html#bayesian-workflow",
    "title": "Generalized Linear Models with brms",
    "section": "Bayesian Workflow",
    "text": "Bayesian Workflow\n\nNice paper about the Bayesian Workflow: https://sites.stat.columbia.edu/gelman/research/unpublished/Bayesian_Workflow_article.pdf"
  },
  {
    "objectID": "slides/bayesian-glm.html#references",
    "href": "slides/bayesian-glm.html#references",
    "title": "Generalized Linear Models with brms",
    "section": "References",
    "text": "References\n\n\n\n\nGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73: 307–9. https://doi.org/10.1080/00031305.2018.1549100.\n\n\nGranziol, Umberto, Maximilian Rabe, Marcello Gallucci, Andrea Spoto, and Giulio Vidotto. 2025. “Not Another Post Hoc Paper: A New Look at Contrast Analysis and Planned Comparisons.” Advances in Methods and Practices in Psychological Science 8 (January). https://doi.org/10.1177/25152459241293110.\n\n\nSchad, Daniel J, Shravan Vasishth, Sven Hohenstein, and Reinhold Kliegl. 2020. “How to Capitalize on a Priori Contrasts in Linear (Mixed) Models: A Tutorial.” Journal of Memory and Language 110 (February): 104038. https://doi.org/10.1016/j.jml.2019.104038.\n\n\nWagenmakers, Eric-Jan, Tom Lodewyckx, Himanshu Kuriyal, and Raoul Grasman. 2010. “Bayesian Hypothesis Testing for Psychologists: A Tutorial on the Savage–Dickey Method.” Cognitive Psychology 60 (May): 158–89. https://doi.org/10.1016/j.cogpsych.2009.12.001.\n\n\nWagenmakers, Eric-Jan, and Alexander Ly. 2023. “History and Nature of the Jeffreys-Lindley Paradox.” Archive for History of Exact Sciences 77: 25–72. https://doi.org/10.1007/s00407-022-00298-3."
  },
  {
    "objectID": "exercises/teddy.html",
    "href": "exercises/teddy.html",
    "title": "Teddy Child",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/filippogambarota/work/teaching-projects/intro-bayesian-statistics-bertinoro\nWe can use as an example the Study conducted by the University of Padua (TEDDY Child Study, 2020)1. Within the study, researchers asked the participants (mothers of a young child) about the presence of post-partum depression and measured the parental stress using the PSI-Parenting Stress Index.\nteddy &lt;- read.csv(here(\"data/clean/teddy_child.csv\"))\nhead(teddy)\n\n  ID Children Fam_status        Education Occupation Age Smoking_status\n1  1        1    Married      High school  Full-time  42             No\n2  2        1    Married      High school  Full-time  42             No\n3  3        1    Married      High school  Full-time  44             No\n4  4        2    Married      High school  Full-time  48             No\n5  5        3    Married Degree or higher  Full-time  39            Yes\n6  6        2    Married Degree or higher  Full-time  38      Ex-smoker\n  Alcool_status Depression_pp Parental_stress Alabama_involvment MSPSS_total\n1            No            No              75                 36    6.500000\n2            No            No              51                 41    5.500000\n3            No            No              76                 42    1.416667\n4            No            No              88                 39    1.083333\n5           Yes            No              67                 35    7.000000\n6            No            No              61                 36    3.333333\n  LTE Partner_cohesion Family_cohesion\n1   5         4.333333        2.916667\n2   5         4.166667        3.000000\n3   4         3.000000        2.500000\n4   4         2.166667        2.666667\n5   2         4.166667        2.666667\n6   0         3.000000        2.583333\nExploring the dataset try to fit Bayesian models predicting the Parental_stress with a set of predictors using additive effects and interactions.\nExploring the dataset try to fit Bayesian models predicting the Depression_pp with a set of predictors using additive effects and interactions."
  },
  {
    "objectID": "exercises/teddy.html#footnotes",
    "href": "exercises/teddy.html#footnotes",
    "title": "Teddy Child",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to Prof. Paolo Girardi for the example, see https://teddychild.dpss.psy.unipd.it/ for information↩︎"
  }
]